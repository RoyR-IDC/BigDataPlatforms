{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73407421-50bb-4ab1-8f8a-8a8833506f9d",
   "metadata": {},
   "source": [
    "# HW 1\n",
    "\n",
    ">Or Livne ID 203972922\n",
    "\n",
    ">Roy Rubin ID 201312907"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28d62c2-259a-4067-b446-c2990cf83f34",
   "metadata": {},
   "source": [
    "## Task 1: CSV and SQL\n",
    "1. Write Python code to create SQLite database “mydb.db” and create a table “mydata” with the schema of the “mydata.csv”\n",
    "2. Write Python code to load “mydata.csv” into “mydata” table.\n",
    "3. Write 2 different SQL statements with different condiJons to retrieve different rows. Explain which parts of the statement are predicate and which parts are projecJon."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66fa103-bd9e-41ff-8888-0fc1778fd580",
   "metadata": {},
   "source": [
    "### Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0a1db88-de14-415b-b8e9-6225623986c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Install pandas\n",
      "\n",
      "Requirement already satisfied: pandas in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (1.3.4)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pandas) (1.21.3)\n",
      "Requirement already satisfied: pytz>=2017.3 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pandas) (2021.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from python-dateutil>=2.7.3->pandas) (1.16.0)\n",
      "\n",
      "Install pyarrow\n",
      "\n",
      "Requirement already satisfied: pyarrow in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (6.0.0)\n",
      "Requirement already satisfied: numpy>=1.16.6 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pyarrow) (1.21.3)\n",
      "\n",
      "Install dask\n",
      "\n",
      "Requirement already satisfied: dask in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (2021.11.0)\n",
      "Requirement already satisfied: pyyaml in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from dask) (6.0)\n",
      "Requirement already satisfied: cloudpickle>=1.1.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from dask) (2.0.0)\n",
      "Requirement already satisfied: toolz>=0.8.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from dask) (0.11.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from dask) (21.0)\n",
      "Requirement already satisfied: partd>=0.3.10 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from dask) (1.2.0)\n",
      "Requirement already satisfied: fsspec>=0.6.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from dask) (2021.11.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from packaging>=20.0->dask) (2.4.7)\n",
      "Requirement already satisfied: locket in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from partd>=0.3.10->dask) (0.2.1)\n",
      "\n",
      "Finished Installations\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(f'Install pandas\\n')\n",
    "!{sys.executable} -m pip install pandas\n",
    "print(f'\\nInstall pyarrow\\n')\n",
    "!{sys.executable} -m pip install pyarrow\n",
    "print(f'\\nInstall dask\\n')\n",
    "!{sys.executable} -m pip install dask\n",
    "print(f'\\nFinished Installations')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef981c94-c24a-487b-a297-4593b53ebd8d",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e293654c-0300-4470-aaa5-56054183f2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import random\n",
    "import os \n",
    "import gc\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "import dask\n",
    "from dask.dataframe import to_parquet\n",
    "import csv\n",
    "from dask.dataframe import from_pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f35eeb-f943-433f-86da-af02a373215d",
   "metadata": {},
   "source": [
    "### Set global definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a57396bb-8713-4b38-9100-820c3c4424e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_data_set_configuration():\n",
    "    # Set the value options\n",
    "    global max_rows, csv_columns, db_file_name, csv_file_name, columns_type\n",
    "    global parquet_file_name_using_dask,parquet_file_name_using_pyarrow \n",
    "    global parquet_file_name_using_pandas\n",
    "    max_rows = 1000000   # NOTE: final size should be 1000000\n",
    "    csv_columns = ['id', 'fruit', 'price', 'color']\n",
    "    columns_type = ['integer', 'text', 'integer', 'text']\n",
    "\n",
    "    db_file_name = 'mydb.db'\n",
    "    csv_file_name = 'mydata.csv'\n",
    "    parquet_file_name_using_dask = 'mydatadask.parquet'\n",
    "    parquet_file_name_using_pyarrow = 'mydatapyarrow.parquet'\n",
    "    parquet_file_name_using_pandas = 'mydatapandas.parquet'\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5da5df02-8005-4cb2-8a0d-a9afd7a583b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init_data_set_configuration settings\n",
    "init_data_set_configuration()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae7cfa4-af29-4f37-9e6c-a82e2843369a",
   "metadata": {},
   "source": [
    "## Preparations for Task1.1\n",
    "create the csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c751d24-fd67-40bd-94ec-4efce106064d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create local CSV file “mydata.csv” with 1000000 \n",
    "rows with columns (id, fruit, price, color). \n",
    "fruit has one of the values ['Orange', 'Grape', 'Apple', 'Banana','Pineapple', 'Avocado'] \n",
    "colors are ['Red', 'Green', 'Yellow', 'Blue']. \n",
    "Price should be random integer between 10 and 100. \n",
    "Filed id should be an index number starting from 1.\n",
    "\"\"\"\n",
    "class single_fruit():\n",
    "    fruit_name = ''\n",
    "    fruit_price = ''\n",
    "    fruit_color = ''\n",
    "    fruit_id = ''\n",
    "    \n",
    "    # Set the value options\n",
    "    fruit_options_list = ['Orange', 'Grape', 'Apple', 'Banana','Pineapple', 'Avocado']\n",
    "    colors_options_list = ['Red', 'Green', 'Yellow', 'Blue']\n",
    "    price_options_range = [10,100]\n",
    "    \n",
    "    def __init__(self, id):\n",
    "        self.fruit_name = random.choice(self.fruit_options_list)\t\n",
    "        self.fruit_price = np.random.randint(self.price_options_range[0], self.price_options_range[1])\n",
    "        self.fruit_color = random.choice(self.colors_options_list)\n",
    "        self.fruit_id = id\n",
    "\n",
    "def create_csvdatabase_file(max_rows):\n",
    "    # this loop generate single fruit\n",
    "    create_n_list_in_advance = max_rows*[None]\n",
    "    for i_row_index in range(max_rows):\n",
    "        i_friut = single_fruit(i_row_index)\n",
    "        create_n_list_in_advance[i_row_index] = [i_friut.fruit_id, i_friut.fruit_name, \n",
    "                                                 i_friut.fruit_price, i_friut.fruit_color]\n",
    "    \n",
    "    mydata_df = pd.DataFrame(create_n_list_in_advance, columns = csv_columns )\n",
    "    mydata_df.to_csv(csv_file_name)\n",
    "    return mydata_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f93ed9b-9533-4dba-a539-efcab40ca308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create csv file as data frame \n",
    "mydata_df = create_csvdatabase_file(max_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ce1167-fc65-472a-88a1-35cdcbaa08fd",
   "metadata": {},
   "source": [
    "## Task1.1: \n",
    "Write Python code to create SQLite database “mydb.db” and create a table “mydata” with the schema of the “mydata.csv”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4480588f-a6c1-4ae6-b4c7-43cabd938753",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_db_and_table(db_file_name):\n",
    "    is_exist = os.path.exists(db_file_name)\n",
    "    if  is_exist:\n",
    "        os.remove(db_file_name)\n",
    "    con = sqlite3.connect(db_file_name)  # This row safely connects to DB and creates the DB if does not exist\n",
    "        \n",
    "    cur = con.cursor()\n",
    "    \n",
    "    # Create table\n",
    "    columns_type_list = list(map(lambda x,y: x+' ' + y, csv_columns, columns_type))\n",
    "    columns_type_list_string = \"(\"+\", \".join(map(str, columns_type_list))+\")\"\n",
    "\n",
    "    cur.execute(''' \n",
    "                CREATE TABLE fruits\n",
    "                ''' + columns_type_list_string + \\\n",
    "               '''''')\n",
    "    \n",
    "    con.commit()\n",
    "    con.close()\n",
    "    return con, cur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0464ef8a-b3b9-489a-9c45-ac08d3c4f35a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.7 ms, sys: 3.41 ms, total: 5.11 ms\n",
      "Wall time: 4.98 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# create a data base and table\n",
    "con, cur = create_db_and_table(db_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc020637-7039-4416-93e1-947e748e5816",
   "metadata": {},
   "source": [
    "## Task1.2: \n",
    "Write Python code to load “mydata.csv” into “mydata” table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03d40f60-41f7-4a61-af37-8981f3ada270",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_db_data_base_using_csv_data_base_with_same_keys(mydata_df):\n",
    "    con = sqlite3.connect(db_file_name)\n",
    "    cur = con.cursor()\n",
    "\n",
    "    for i_csv_row in range(mydata_df.shape[0]):\n",
    "        # Take row from csv file\n",
    "        i_row = mydata_df.iloc[i_csv_row]\n",
    "        i_row_as_list = i_row.to_list()\n",
    "        \n",
    "        i_row_as_list_string = \"('\"+\"','\".join(map(str, i_row_as_list))+\"')\"\n",
    "        #cur.execute(\"INSERT INTO stocks VALUES ('2006-01-05','BUY','RHAT','100')\")\n",
    "\n",
    "        #Insert a csv row of data base\n",
    "        cur.execute(\"INSERT INTO fruits VALUES \"  \\\n",
    "                    +i_row_as_list_string + \\\n",
    "                    \"\")\n",
    "    con.commit()\n",
    "    con.close()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "773f2ea7-a7eb-4f0b-a1a4-581ae76a0b5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 2s, sys: 809 ms, total: 2min 3s\n",
      "Wall time: 2min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# copy csv data into db file\n",
    "fill_db_data_base_using_csv_data_base_with_same_keys(mydata_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3afb4fab-a883-4816-a096-0d2a560c0512",
   "metadata": {},
   "source": [
    "## Task1.3: \n",
    "Write 2 different SQL statements with different conditions to retrieve different rows. \n",
    "Explain which parts of the statement are predicate and which parts are projection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8829619d-2bc1-4c50-a172-6161c1c3df04",
   "metadata": {},
   "source": [
    "### 0) Prepare print functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a7676f8f-72a9-4e4c-ab39-897c6987f914",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_db_file_multiplecolumns_base_one_column_and_equal_value(key, value, columns:list = [], num_rows_to_print:int = 3):\n",
    "    con = sqlite3.connect(db_file_name)\n",
    "    cur = con.cursor()\n",
    "    key_index = csv_columns.index(key)\n",
    "    key_type = columns_type[key_index]\n",
    "    \n",
    "    if columns is None or len(columns) == 0:\n",
    "        columns = '*'\n",
    "    else:\n",
    "        columns = ','.join(columns)\n",
    "    \n",
    "    if key_type == 'integer':\n",
    "        query = f'SELECT {columns} FROM fruits  WHERE {key} = {value}'\n",
    "    else:\n",
    "        query = f'SELECT {columns} FROM fruits  WHERE {key} = \\'{value}\\''\n",
    "    \n",
    "    for index, row in enumerate(cur.execute(query)):\n",
    "        if index >= num_rows_to_print:\n",
    "            break\n",
    "        print(row)\n",
    "\n",
    "    con.close()\n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1fa660d-b02d-4d3a-acb0-3d429602e76f",
   "metadata": {},
   "source": [
    "### 1) Predicate Operation : \n",
    "\n",
    "> This operation is used to select rows from a table (relation) that specifies a given logic, which is called as a predicate. The predicate is a user defined condition to select rows of user's choice.\n",
    "\n",
    "> we used the 4 functions above to filter out rows in which different condition were applied\n",
    "\n",
    "> inside the sql query, the query section \"... WHERE {key} = {value}\" is what actually filters the wanted rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e8e285ec-db2c-4fb1-b6da-81d1007ae395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "print only rows in which the color equals red\n",
      "(3, 'Avocado', 35, 'Red')\n",
      "(8, 'Avocado', 18, 'Red')\n",
      "(13, 'Banana', 22, 'Red')\n"
     ]
    }
   ],
   "source": [
    "print(f'print only rows in which the color equals red')\n",
    "print_db_file_multiplecolumns_base_one_column_and_equal_value('color', 'Red')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f258a0b4-2201-411e-a27d-617e65a04727",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2) Project Operation : \n",
    "\n",
    "> If the user is interested in selecting the values of a few attributes, rather than selection all attributes of the Table then one should go for PROJECT Operation.  \n",
    "        \n",
    "> We used the function above to prject only 2 columns (and also filter some rows)\n",
    "\n",
    "> inside the sql query, the query section \"SELECT {list of columns} FROM ...\" is what actually filters the wanted columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "be47f06c-c834-4d9a-8fd3-0c8db80aade2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "print only the color and id columns from the rows in which the color equals blue\n",
      "('Blue', 0)\n",
      "('Blue', 7)\n",
      "('Blue', 9)\n"
     ]
    }
   ],
   "source": [
    "print(f'print only the color and id columns from the rows in which the color equals blue')\n",
    "print_db_file_multiplecolumns_base_one_column_and_equal_value('color', 'Blue', ['color', 'id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f5c80c-623e-4e16-a832-da7ea602d81b",
   "metadata": {},
   "source": [
    "## Task 2: CSV and Parquet\n",
    "1. Write Python program that reads “mydata.csv” file and count number of lines\n",
    "2. By using PyArrow, create Parquet file from the “mydata.csv”. Name Parquet file as “mydatapyarrow.parquet”\n",
    "3. By using Dask, create Parquet file from the “mydata.csv”. Name Parquet file as “mydatadask.parquet”\n",
    "4. By using Pandas, create Parquet file from the “mydata.csv”. Name Parquet file as “mydatapandas.parquet”\n",
    "5. Examine generated Parquet files. Why do you think Dask generated Parquet file differently than PyArrow and Pandas? What might be explanaJon for this?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24208dd3-ba4c-40dc-8e53-19d296ebac07",
   "metadata": {},
   "source": [
    "## Task 2.1:\n",
    "Write Python program that reads “mydata.csv” file and count number of lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3861ee48-0382-4fa5-aae8-727de60ff78f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.45 s, sys: 147 ms, total: 2.6 s\n",
      "Wall time: 2.64 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# read csv file \n",
    "with open(csv_file_name) as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    line_count = 0\n",
    "    csv_rows = []\n",
    "    for row in csv_reader:\n",
    "        if line_count == 0:\n",
    "            columns_list = row[1::]\n",
    "        else:\n",
    "            current_row = row[1::]\n",
    "            csv_rows.append(current_row)\n",
    "        line_count+=1\n",
    "\n",
    "mydata_df = pd.DataFrame(csv_rows, columns = columns_list )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5f03fce5-a0bf-44d9-a5d7-1d8acb5b7e06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The amount of row in the following csv is when counting with headers: 1000001\n",
      "The amount of row in the following csv is when counting without headers: 1000000\n"
     ]
    }
   ],
   "source": [
    "print('The amount of row in the following csv is when counting with headers: ' + str(line_count))\n",
    "print('The amount of row in the following csv is when counting without headers: ' + str(len(mydata_df.index)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cea462-4d26-47dc-b7f7-7cfcc4de21a7",
   "metadata": {},
   "source": [
    "## Task 2.2:\n",
    "By using PyArrow, create Parquet file from the “mydata.csv”. Name Parquet file as “mydatapyarrow.parquet”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "27e22bb1-35b1-43f3-8df7-cec220ab2ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyarray_table = pa.Table.from_pandas(mydata_df)\n",
    "pq.write_table(pyarray_table, parquet_file_name_using_pyarrow)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449d8b99-eb5c-4b9e-9245-5e8aa916203b",
   "metadata": {},
   "source": [
    "## Task 2.3\n",
    "By using Dask, create Parquet file from the “mydata.csv”. Name Parquet file as “mydatadask.parquet”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ee04d150-23e0-4d76-a48b-94f4ddfbb62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_df = from_pandas(mydata_df, npartitions=4)\n",
    "dask_df.to_parquet(parquet_file_name_using_dask);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97fef60-4f9a-45f4-978b-ec88d8dfece4",
   "metadata": {},
   "source": [
    "## Task 2.4\n",
    "By using Pandas, create Parquet file from the “mydata.csv”. Name Parquet file as “mydatapandas.parquet”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4f7f195e-2847-4cb1-be83-87bf23dc9042",
   "metadata": {},
   "outputs": [],
   "source": [
    "mydata_df.to_parquet(parquet_file_name_using_pandas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95bfdf45-5812-45ad-beaa-57130840a762",
   "metadata": {},
   "source": [
    "## Task 2.5\n",
    "Examine generated Parquet files. Why do you think Dask generated Parquet file differently than PyArrow and Pandas? What might be explanation for this?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f1b104-0d29-4f68-a9ea-ea2d847188a1",
   "metadata": {},
   "source": [
    "### Task 2.5 answer\n",
    "\n",
    "> Unlke parquet files generated from pyarrow and pandas which are saved as a single parquet file, when conerting to parquet using dask we get a folder.\n",
    "\n",
    "> the parquet file generated from dask is separated into the predefined number of partitions (if we defined npartitions=4, then our dataframe will be split into 4 datasets, which is later saved as 4 parquet files)\n",
    "\n",
    "> the main eason dask splits the data into 4 sets, is to allow parrallel computing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9c413c-53fa-4e38-b128-be48931c3cd0",
   "metadata": {},
   "source": [
    "## Task 3: Split CSV files\n",
    "1. Write Python code that calculates size of “mydata.csv” in bytes. Define an integer variable “middle” which is the size of “mydata.csv” divided by 2.\n",
    "2. Write a Python funcJon first_chunk that count number of rows by reading the byte range of the CSV file, from 0 Jll the “middle”. Write a funcJon last_chunk that count number of rows by reading byte range of CSV file from the “middle”+1 Jll the end of the file.\n",
    "3. Explain why total number of lines from the first chunk and second chunk is larger than the number of lines calculated in the step (1) of Task 2.\n",
    "4. Suggest an algorithm to resolve the issue from the step (3) and implement it.\n",
    "5. Check the algorithm of step (4) with mulJple chunks. Define a chunk size to be 16MB. Write a funcJon that process “mydata.csv “ in chunks and count number of lines for each chunk. For example, first chunk will be 0-16MB, second chunk 16MB-32BM, and so on, until the last chunk, which might be smaller."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3643196a-3800-4e41-9634-3ba6b63473f7",
   "metadata": {},
   "source": [
    "## Task 3.1\n",
    "Write Python code that calculates size of “mydata.csv” in bytes. Define an integer variable “middle” which is the size of “mydata.csv” divided by 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b45879e5-a500-48af-824a-c2baf36dbf3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory_usage_size of file 29611308 bytes\n",
      "middle of file 14805654 bytes\n"
     ]
    }
   ],
   "source": [
    "memory_usage_size = os.path.getsize(csv_file_name)\n",
    "middle = memory_usage_size//2\n",
    "print(f'memory_usage_size of file {memory_usage_size} bytes')\n",
    "print(f'middle of file {middle} bytes')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e6e684-216b-4ab7-8450-8c93a60a1c64",
   "metadata": {},
   "source": [
    "## Task 3.2\n",
    "Write a Python function first_chunk that count number of rows by reading the byte range of the CSV file, from 0 until the “middle”. Write a funcJon last_chunk that count number of rows by reading byte range of CSV file from the “middle”+1 untill the end of the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "54768ba8-2266-4025-b97e-03ef8a0fa3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_chunk():\n",
    "    f2 = open(csv_file_name, \"rb\")\n",
    "    f2.seek(0, 0)\n",
    "    d2 = f2.read(middle).decode(encoding='utf-8')\n",
    "    amount_of_rows = d2.count('\\n')\n",
    "    \n",
    "    was_endline = False\n",
    "    if d2[-1] != '\\n':\n",
    "        amount_of_rows +=1\n",
    "        was_endline = True\n",
    "        \n",
    "    return amount_of_rows, was_endline\n",
    "\n",
    "def last_chunk():\n",
    "    f2 = open(csv_file_name, \"rb\")\n",
    "    f2.seek(middle+1, 0)\n",
    "    d2 = f2.read(middle).decode(encoding='utf-8')\n",
    "    amount_of_rows = d2.count('\\n')\n",
    "\n",
    "    return amount_of_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "217e2e3a-135d-4585-96ae-2752f8167196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amount_of_rows_first 503738\n",
      "amount_of_rows_last 496264\n",
      "total 1000002\n"
     ]
    }
   ],
   "source": [
    "amount_of_rows_first,_ = first_chunk()\n",
    "amount_of_rows_last = last_chunk()\n",
    "print(f'amount_of_rows_first {amount_of_rows_first}')\n",
    "print(f'amount_of_rows_last {amount_of_rows_last}')\n",
    "print(f'total {amount_of_rows_first + amount_of_rows_last}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea313a86-5067-4896-9aa1-a673b462f3fb",
   "metadata": {},
   "source": [
    "## Task 3.3\n",
    "Explain why total number of lines from the first chunk and second chunk is larger than the number of lines calculated in the step (1) of Task 2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d773db-6e84-4daf-8de6-5823a8aea0ce",
   "metadata": {},
   "source": [
    "### Task 3.3 answer\n",
    "\n",
    "> in this task, the file chunk might end in the middle of a row, and so that row could be counted twice.\n",
    "\n",
    "> we also note that it matters if you count the number of rows with or without headers of the file. we decided by convention to count with headers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934f2155-0345-4445-ac2a-1249ba5ab5fc",
   "metadata": {},
   "source": [
    "## Task 3.4\n",
    "Suggest an algorithm to resolve the issue from the step (3) and implement it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9b2e324a-84e8-4642-8778-1750cc5a45a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def current_chunk(location, chunk_size):\n",
    "    f2 = open(csv_file_name, \"rb\")\n",
    "    f2.seek(location, 0)\n",
    "    d2 = f2.read(chunk_size).decode(encoding='utf-8')\n",
    "    amount_of_rows = d2.count('\\n')\n",
    "    \n",
    "    was_endline = False\n",
    "    if d2[-1] != '\\n':\n",
    "        amount_of_rows +=1\n",
    "        was_endline = True\n",
    "        \n",
    "    return amount_of_rows, was_endline\n",
    "\n",
    "def get_number_of_lines_in_file_v2(file_size, chunk_size:int = 33):\n",
    "    total_rows = 0\n",
    "    for location in range(0,file_size,chunk_size):    \n",
    "        amount_of_rows, was_endline = current_chunk(location, chunk_size)\n",
    "        total_rows += amount_of_rows\n",
    "        if was_endline is True:\n",
    "            total_rows -= 1\n",
    "        \n",
    "    return total_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c524939-5f65-4db0-a886-3af7b6cf743d",
   "metadata": {},
   "source": [
    "## Task 3.5\n",
    "Check the algorithm of step (4) with multiple chunks. Define a chunk size to be 16MB. Write a function that process “mydata.csv “ in chunks and count number of lines for each chunk. For example, first chunk will be 0-16MB, second chunk 16MB-32BM, and so on, until the last chunk, which might be smaller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0e28de5c-406c-4de9-9f45-67283cc80f8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total amount of rows 1000001\n",
      "CPU times: user 60.2 ms, sys: 40.8 ms, total: 101 ms\n",
      "Wall time: 102 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "chunk_size = 16000\n",
    "result = get_number_of_lines_in_file_v2(memory_usage_size, chunk_size)\n",
    "print(f'total amount of rows {result}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8382643-5d86-4a3b-a090-d9f5ebddca80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
