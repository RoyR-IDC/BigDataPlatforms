{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73407421-50bb-4ab1-8f8a-8a8833506f9d",
   "metadata": {},
   "source": [
    "# HW 1\n",
    "\n",
    ">Or Livne ID 203972922\n",
    "\n",
    ">Roy Rubin ID 201312907"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28d62c2-259a-4067-b446-c2990cf83f34",
   "metadata": {},
   "source": [
    "## Task 1: CSV and SQL\n",
    "1. Write Python code to create SQLite database “mydb.db” and create a table “mydata” with the schema of the “mydata.csv”\n",
    "2. Write Python code to load “mydata.csv” into “mydata” table.\n",
    "3. Write 2 different SQL statements with different condiJons to retrieve different rows. Explain which parts of the statement are predicate and which parts are projecJon."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66fa103-bd9e-41ff-8888-0fc1778fd580",
   "metadata": {},
   "source": [
    "## imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e293654c-0300-4470-aaa5-56054183f2ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import random\n",
    "import os \n",
    "import gc\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "import dask\n",
    "from dask.dataframe import to_parquet\n",
    "import csv\n",
    "from dask.dataframe import from_pandas\n",
    "import gc \n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d55c81-50f7-44af-b642-33227e4c73e2",
   "metadata": {},
   "source": [
    "## define function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b349006d-59d2-4c17-8696-1919ea307bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create local CSV file “mydata.csv” with 1000000 \n",
    "rows with columns (id, fruit, price, color). \n",
    "fruit has one of the values ['Orange', 'Grape', 'Apple', 'Banana','Pineapple', 'Avocado'] \n",
    "colors are ['Red', 'Green', 'Yellow', 'Blue']. \n",
    "Price should be random integer between 10 and 100. \n",
    "Filed id should be an index number starting from 1.\n",
    "\"\"\"\n",
    "class single_fruit():\n",
    "    fruit_name = ''\n",
    "    fruit_price = ''\n",
    "    fruit_color = ''\n",
    "    fruit_id = ''\n",
    "    \n",
    "    # Set the value options\n",
    "    fruit_options_list = ['Orange', 'Grape', 'Apple', 'Banana','Pineapple', 'Avocado']\n",
    "    colors_options_list = ['Red', 'Green', 'Yellow', 'Blue']\n",
    "    price_options_range = [10,100]\n",
    "    \n",
    "    def __init__(self, id):\n",
    "        self.fruit_name = random.choice(self.fruit_options_list)\t\n",
    "        self.fruit_price = np.random.randint(self.price_options_range[0], self.price_options_range[1])\n",
    "        self.fruit_color = random.choice(self.colors_options_list)\n",
    "        self.fruit_id = id\n",
    "        \n",
    "\n",
    "def init_data_set_configuration():\n",
    "    # Set the value options\n",
    "    global max_rows, csv_columns, db_file_name, csv_file_name, columns_type\n",
    "    global parquet_file_name_using_dask,parquet_file_name_using_pyarray \n",
    "    global parquet_file_name_using_pandas\n",
    "    max_rows = 10\n",
    "    csv_columns = ['id', 'fruit', 'price', 'color']\n",
    "    columns_type = ['integer', 'text', 'integer', 'text']\n",
    "\n",
    "    db_file_name = 'mydata.db'\n",
    "    csv_file_name = 'mydata.csv'\n",
    "    parquet_file_name_using_dask = 'mydatapyarrow_dask.parquet'\n",
    "    parquet_file_name_using_pyarray = 'mydatapyarrow_pyarray.parquet'\n",
    "    parquet_file_name_using_pandas = 'mydatapyarrow_pandas.parquet'\n",
    "\n",
    "    return\n",
    "\n",
    "def create_csvdatabase_file(max_rows):\n",
    "    # this loop generate single fruit\n",
    "    create_n_list_in_advance = max_rows*[None]\n",
    "    for i_row_index in range(max_rows):\n",
    "        i_friut = single_fruit(i_row_index)\n",
    "        create_n_list_in_advance[i_row_index] = [i_friut.fruit_id, i_friut.fruit_name, \n",
    "                                                 i_friut.fruit_price, i_friut.fruit_color]\n",
    "    \n",
    "    mydata_df = pd.DataFrame(create_n_list_in_advance, columns = csv_columns )\n",
    "    mydata_df.to_csv(csv_file_name)\n",
    "    return mydata_df\n",
    "\n",
    "def create_db_database(db_file_name):\n",
    "    is_exist = os.path.exists(db_file_name)\n",
    "    if  is_exist:\n",
    "        os.remove(db_file_name)\n",
    "    con = sqlite3.connect(db_file_name)\n",
    "        \n",
    "    \n",
    "    cur = con.cursor()\n",
    "    \n",
    "    # Create table\n",
    "    columns_type_list = list(map(lambda x,y: x+' ' + y, csv_columns, columns_type))\n",
    "    columns_type_list_string = \"(\"+\", \".join(map(str, columns_type_list))+\")\"\n",
    "\n",
    "    cur.execute(''' \n",
    "                CREATE TABLE stocks\n",
    "                ''' + columns_type_list_string + \\\n",
    "               '''''')\n",
    "    \n",
    "    con.commit()\n",
    "    con.close()\n",
    "    return con, cur\n",
    "\n",
    "def fill_db_data_base_using_csv_data_base_with_same_keys(mydata_df):\n",
    "    con = sqlite3.connect(db_file_name)\n",
    "    cur = con.cursor()\n",
    "\n",
    "    for i_csv_row in range(mydata_df.shape[0]):\n",
    "        # Take row from csv file\n",
    "        i_row = mydata_df.iloc[i_csv_row]\n",
    "        i_row_as_list = i_row.to_list()\n",
    "        \n",
    "        i_row_as_list_string = \"('\"+\"','\".join(map(str, i_row_as_list))+\"')\"\n",
    "        #cur.execute(\"INSERT INTO stocks VALUES ('2006-01-05','BUY','RHAT','100')\")\n",
    "\n",
    "        #Insert a csv row of data base\n",
    "        cur.execute(\"INSERT INTO stocks VALUES \"  \\\n",
    "                    +i_row_as_list_string + \\\n",
    "                    \"\")\n",
    "    con.commit()\n",
    "    con.close()\n",
    "    return\n",
    "\n",
    "def print_db_file_info_bsase_single_key(key):\n",
    "    con = sqlite3.connect(db_file_name)\n",
    "    cur = con.cursor()\n",
    "    for row in cur.execute('SELECT * FROM stocks ORDER BY '+ key):\n",
    "        print(row)\n",
    "    con.close()\n",
    "    return\n",
    "\n",
    "    \n",
    "def print_db_file_info_base_one_column_and_equal_value(key, value):\n",
    "    con = sqlite3.connect(db_file_name)\n",
    "    cur = con.cursor()\n",
    "    key_index = csv_columns.index(key)\n",
    "    key_type = columns_type[key_index]\n",
    "    \n",
    "    if key_type == 'integer':\n",
    "        query = f'SELECT * FROM stocks  WHERE {key} = {value}'\n",
    "    else:\n",
    "        query = f'SELECT * FROM stocks  WHERE {key} = \\'{value}\\''\n",
    "    \n",
    "    for row in cur.execute(query):\n",
    "        print(row)\n",
    "\n",
    "    con.close()\n",
    "    return \n",
    "\n",
    "def print_db_file_info_base_one_column_and_equal_value2(key, value):\n",
    "    con = sqlite3.connect(db_file_name)\n",
    "    cur = con.cursor()\n",
    "    key_index = csv_columns.index(key)\n",
    "    key_type = columns_type[key_index]\n",
    "    \n",
    "    if key_type == 'integer':\n",
    "        query = f'SELECT FROM stocks  WHERE {key} = {value}'\n",
    "    else:\n",
    "        query = f'SELECT FROM stocks  WHERE {key} = \\'{value}\\''\n",
    "    \n",
    "    for row in cur.execute(query):\n",
    "        print(row)\n",
    "\n",
    "    con.close()\n",
    "    return \n",
    "\n",
    "def print_db_file_multiplecolumns_base_one_column_and_equal_value(key, value, columns:list):\n",
    "    con = sqlite3.connect(db_file_name)\n",
    "    cur = con.cursor()\n",
    "    key_index = csv_columns.index(key)\n",
    "    key_type = columns_type[key_index]\n",
    "    \n",
    "    if columns is None or len(columns) == 0:\n",
    "        columns = '*'\n",
    "    else:\n",
    "        columns = ','.join(columns)\n",
    "    \n",
    "    if key_type == 'integer':\n",
    "        query = f'SELECT {columns} FROM stocks  WHERE {key} = {value}'\n",
    "    else:\n",
    "        query = f'SELECT {columns} FROM stocks  WHERE {key} = \\'{value}\\''\n",
    "    \n",
    "    for row in cur.execute(query):\n",
    "        print(row)\n",
    "\n",
    "    con.close()\n",
    "    return \n",
    "\n",
    "def print_db_file_info_base_one_column_and_geater_than_value(key, value):\n",
    "    con = sqlite3.connect(db_file_name)\n",
    "    cur = con.cursor()\n",
    "    key_index = csv_columns.index(key)\n",
    "    key_type = columns_type[key_index]\n",
    "    for row in cur.execute('SELECT * FROM stocks  WHERE '+key+'>'+\"%s\"  % value):\n",
    "        print(row)\n",
    "    con.close()\n",
    "    return \n",
    "\n",
    "def print_db_file_info_base_one_column_and_lower_than_value(key, value):\n",
    "    con = sqlite3.connect(db_file_name)\n",
    "    cur = con.cursor()\n",
    "    key_index = csv_columns.index(key)\n",
    "    key_type = columns_type[key_index]\n",
    "    for row in cur.execute('SELECT * FROM stocks  WHERE '+key+'<'+\"%s\"  % value):\n",
    "        print(row)\n",
    "    con.close()\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deee8f55-c371-45b8-b1a8-139c0a313fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ce1167-fc65-472a-88a1-35cdcbaa08fd",
   "metadata": {},
   "source": [
    "## Task1.1: \n",
    "Write Python code to create SQLite database “mydb.db” and create a table “mydata” with the schema of the “mydata.csv”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0464ef8a-b3b9-489a-9c45-ac08d3c4f35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init_data_set_configuration settings\n",
    "init_data_set_configuration()\n",
    "\n",
    "# create csv file as data frame \n",
    "mydata_df = create_csvdatabase_file(max_rows)\n",
    "\n",
    "# create a db data base\n",
    "con, cur = create_db_database(db_file_name)\n",
    "\n",
    "# copy csv data into db file\n",
    "fill_db_data_base_using_csv_data_base_with_same_keys(mydata_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc020637-7039-4416-93e1-947e748e5816",
   "metadata": {},
   "source": [
    "## Task1.2: \n",
    "Write Python code to load “mydata.csv” into “mydata” table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45304242-ae07-4b4c-a980-ac4c4c3aa445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The amount of row in the following csv is 11\n"
     ]
    }
   ],
   "source": [
    "# read csv file \n",
    "with open(csv_file_name) as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    line_count = 0\n",
    "    csv_rows = []\n",
    "    for row in csv_reader:\n",
    "        if line_count == 0:\n",
    "            columns_list = row[1::]\n",
    "        else:\n",
    "            current_row = row[1::]\n",
    "            csv_rows.append(current_row)\n",
    "        line_count+=1\n",
    "\n",
    "# create new data frame base csv file \n",
    "mydata_df_with_out_pandas = pd.DataFrame(csv_rows, columns = columns_list )\n",
    "print('The amount of row in the following csv is ' + str(line_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3afb4fab-a883-4816-a096-0d2a560c0512",
   "metadata": {},
   "source": [
    "## Task1.3: \n",
    "Write 2 different SQL statements with different condiJons to retrieve different rows. Explain which parts of the statement are predicate and which parts are projecJon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877b226e-3b67-46fd-b5a1-72ae20afdf25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fead5da7-6824-4fb8-885a-a2ccac4069a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "print_db_file_info_bsase_single_key\n",
      "(0, 'Apple', 74, 'Red')\n",
      "(1, 'Avocado', 82, 'Green')\n",
      "(2, 'Pineapple', 43, 'Blue')\n",
      "(3, 'Grape', 99, 'Yellow')\n",
      "(4, 'Pineapple', 14, 'Yellow')\n",
      "(5, 'Grape', 76, 'Green')\n",
      "(6, 'Grape', 57, 'Yellow')\n",
      "(7, 'Orange', 32, 'Green')\n",
      "(8, 'Apple', 61, 'Green')\n",
      "(9, 'Apple', 48, 'Red')\n",
      "print_db_file_info_base_one_column_and_equal_value\n",
      "(2, 'Pineapple', 43, 'Blue')\n",
      "print_db_file_info_base_one_column_and_geater_than_value\n",
      "(3, 'Grape', 99, 'Yellow')\n",
      "(4, 'Pineapple', 14, 'Yellow')\n",
      "(5, 'Grape', 76, 'Green')\n",
      "(6, 'Grape', 57, 'Yellow')\n",
      "(7, 'Orange', 32, 'Green')\n",
      "(8, 'Apple', 61, 'Green')\n",
      "(9, 'Apple', 48, 'Red')\n",
      "print_db_file_info_base_one_column_and_lower_than_value\n",
      "(0, 'Apple', 74, 'Red')\n",
      "(1, 'Avocado', 82, 'Green')\n"
     ]
    }
   ],
   "source": [
    "# print db base single key\n",
    "print('print_db_file_info_bsase_single_key')\n",
    "print_db_file_info_bsase_single_key('id')\n",
    "\n",
    "# 3 option 2 query our data base predicate and projection\n",
    "print('print_db_file_info_base_one_column_and_equal_value')\n",
    "print_db_file_info_base_one_column_and_equal_value('color', 'Blue')\n",
    "print('print_db_file_info_base_one_column_and_geater_than_value')\n",
    "print_db_file_info_base_one_column_and_geater_than_value('id',2)\n",
    "print('print_db_file_info_base_one_column_and_lower_than_value')\n",
    "print_db_file_info_base_one_column_and_lower_than_value('id',2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a16747-66fc-4d2d-b358-a494578ecdc8",
   "metadata": {},
   "source": [
    "## 1) Predicate Operation : \n",
    "\n",
    "       a)   This operation is used to select rows from a\n",
    "            table (relation) that specifies a given logic, which is called as\n",
    "            a predicate. \n",
    "       b)  The predicate is a user defined condition to select\n",
    "           rows of user's choice.\n",
    "\n",
    "> we used the 4 functions above to filter out rows in which different condition were applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be47f06c-c834-4d9a-8fd3-0c8db80aade2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_db_file_multiplecolumns_base_one_column_and_equal_value('color', 'Blue', ['color', 'id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f258a0b4-2201-411e-a27d-617e65a04727",
   "metadata": {},
   "source": [
    "## 2) Project Operation : \n",
    "\n",
    "        a)  If the user is interested in selecting the values\n",
    "            of a few attributes, rather than selection all attributes of the Table\n",
    "            then one should go for PROJECT Operation.  \n",
    "        \n",
    "        \n",
    "> Projection: we used the function above to prject only 2 columns (and also filter some rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f5c80c-623e-4e16-a832-da7ea602d81b",
   "metadata": {},
   "source": [
    "## Task 2: CSV and Parquet\n",
    "1. Write Python program that reads “mydata.csv” file and count number of lines\n",
    "2. By using PyArrow, create Parquet file from the “mydata.csv”. Name Parquet file as “mydatapyarrow.parquet”\n",
    "3. By using Dask, create Parquet file from the “mydata.csv”. Name Parquet file as “mydatadask.parquet”\n",
    "4. By using Pandas, create Parquet file from the “mydata.csv”. Name Parquet file as “mydatapandas.parquet”\n",
    "5. Examine generated Parquet files. Why do you think Dask generated Parquet file differently than PyArrow and Pandas? What might be explanaJon for this?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24208dd3-ba4c-40dc-8e53-19d296ebac07",
   "metadata": {},
   "source": [
    "## Task 2.1:\n",
    "Write Python program that reads “mydata.csv” file and count number of lines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cea462-4d26-47dc-b7f7-7cfcc4de21a7",
   "metadata": {},
   "source": [
    "## Task 2.2:\n",
    "By using PyArrow, create Parquet file from the “mydata.csv”. Name Parquet file as"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449d8b99-eb5c-4b9e-9245-5e8aa916203b",
   "metadata": {},
   "source": [
    "## Task 2.3\n",
    "By using Dask, create Parquet file from the “mydata.csv”. Name Parquet file as “mydatadask.parquet”"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97fef60-4f9a-45f4-978b-ec88d8dfece4",
   "metadata": {},
   "source": [
    "## Task 2.4\n",
    "By using Pandas, create Parquet file from the “mydata.csv”. Name Parquet file as “mydatapandas.parquet”"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95bfdf45-5812-45ad-beaa-57130840a762",
   "metadata": {},
   "source": [
    "## Task 2.5\n",
    "Examine generated Parquet files. Why do you think Dask generated Parquet file differently than PyArrow and Pandas? What might be explanaJon for this?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9c413c-53fa-4e38-b128-be48931c3cd0",
   "metadata": {},
   "source": [
    "## Task 3: Split CSV files\n",
    "1. Write Python code that calculates size of “mydata.csv” in bytes. Define an integer variable “middle” which is the size of “mydata.csv” divided by 2.\n",
    "2. Write a Python funcJon first_chunk that count number of rows by reading the byte range of the CSV file, from 0 Jll the “middle”. Write a funcJon last_chunk that count number of rows by reading byte range of CSV file from the “middle”+1 Jll the end of the file.\n",
    "3. Explain why total number of lines from the first chunk and second chunk is larger than the number of lines calculated in the step (1) of Task 2.\n",
    "4. Suggest an algorithm to resolve the issue from the step (3) and implement it.\n",
    "5. Check the algorithm of step (4) with mulJple chunks. Define a chunk size to be 16MB. Write a funcJon that process “mydata.csv “ in chunks and count number of lines for each chunk. For example, first chunk will be 0-16MB, second chunk 16MB-32BM, and so on, unJl the last chunk, which might be smaller."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3643196a-3800-4e41-9634-3ba6b63473f7",
   "metadata": {},
   "source": [
    "## Task 3.1\n",
    "Write Python code that calculates size of “mydata.csv” in bytes. Define an integer variable “middle” which is the size of “mydata.csv” divided by 2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e6e684-216b-4ab7-8450-8c93a60a1c64",
   "metadata": {},
   "source": [
    "## Task 3.2\n",
    "Write a Python funcJon first_chunk that count number of rows by reading the byte range of the CSV file, from 0 Jll the “middle”. Write a funcJon last_chunk that count number of rows by reading byte range of CSV file from the “middle”+1 Jll the end of the file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea313a86-5067-4896-9aa1-a673b462f3fb",
   "metadata": {},
   "source": [
    "## Task 3.3\n",
    "Explain why total number of lines from the first chunk and second chunk is larger than the number of lines calculated in the step (1) of Task 2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934f2155-0345-4445-ac2a-1249ba5ab5fc",
   "metadata": {},
   "source": [
    "## Task 3.4\n",
    "Suggest an algorithm to resolve the issue from the step (3) and implement it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c524939-5f65-4db0-a886-3af7b6cf743d",
   "metadata": {},
   "source": [
    "## Task 3.5\n",
    "Check the algorithm of step (4) with mulJple chunks. Define a chunk size to be 16MB. Write a funcJon that process “mydata.csv “ in chunks and count number of lines for each chunk. For example, first chunk will be 0-16MB, second chunk 16MB-32BM, and so on, unJl the last chunk, which might be smaller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e28de5c-406c-4de9-9f45-67283cc80f8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8382643-5d86-4a3b-a090-d9f5ebddca80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84aec719-90f9-4f61-9868-165f38c5ba2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee26a20c-c9b9-4392-b243-2bd83ea41894",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a62cf2d-1d0b-4b29-9a39-3c5296795891",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
