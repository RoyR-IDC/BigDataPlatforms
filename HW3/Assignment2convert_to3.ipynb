{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big Data Platform\n",
    "## Assignment 2: MapReduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**By:**  \n",
    "\n",
    "Or Livne, 300123123  \n",
    "Roy Rubin, 201312907\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The goal of this assignment is to:**\n",
    "- Understand and practice the details of MapReduceEngine\n",
    "\n",
    "**Instructions:**\n",
    "- Students will form teams of two people each, and submit a single homework for each team.\n",
    "- The same score for the homework will be given to each member of your team.\n",
    "- Your solution is in the form of a Jupyter notebook file (with extension ipynb).\n",
    "- Images/Graphs/Tables should be submitted inside the notebook.\n",
    "- The notebook should be runnable and properly documented. \n",
    "- Please answer all the questions and include all your code.\n",
    "- You are expected to submit a clear and pythonic code.\n",
    "- You can change functions signatures/definitions.\n",
    "\n",
    "**Submission:**\n",
    "- Submission of the homework will be done via Moodle by uploading a Jupyter notebook.\n",
    "- The homework needs to be entirely in English.\n",
    "- The deadline for submission is on Moodle.\n",
    "- Late submission won't be allowed.\n",
    "  \n",
    "  \n",
    "- In case of identical code submissions - both groups will get a Zero. \n",
    "- Some groups might be selected randomly to present their code.\n",
    "\n",
    "**Requirements:**  \n",
    "- Python 3.6 should be used.  \n",
    "- You should implement the algorithms by yourself using only basic Python libraries (such as numpy,pandas,etc.)\n",
    "\n",
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Grading:**\n",
    "- Q1 - 5 points - Initial Steps\n",
    "- Q2 - 50 points - MapReduceEngine\n",
    "- Q3 - 30 points - Implement the MapReduce Inverted index of the JSON documents\n",
    "- Q4 - 5 points - Testing Your MapReduce\n",
    "- Q5 - 10 points - Final Thoughts \n",
    "\n",
    "`Total: 100`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prerequisites**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Install joblib\n",
      "\n",
      "Requirement already satisfied: joblib in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (1.1.0)\n",
      "\n",
      "Install lithops\n",
      "\n",
      "Requirement already satisfied: lithops in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (2.5.8)\n",
      "Requirement already satisfied: redis in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from lithops) (4.1.0)\n",
      "Requirement already satisfied: ps-mem in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from lithops) (3.12)\n",
      "Requirement already satisfied: cloudpickle in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from lithops) (2.0.0)\n",
      "Requirement already satisfied: python-dateutil in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from lithops) (2.8.2)\n",
      "Requirement already satisfied: paramiko in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from lithops) (2.9.1)\n",
      "Requirement already satisfied: pika in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from lithops) (1.2.0)\n",
      "Requirement already satisfied: seaborn in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from lithops) (0.11.2)\n",
      "Requirement already satisfied: matplotlib in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from lithops) (3.4.3)\n",
      "Requirement already satisfied: Click in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from lithops) (8.0.3)\n",
      "Requirement already satisfied: pandas in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from lithops) (1.3.4)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from lithops) (2.26.0)\n",
      "Requirement already satisfied: PyYAML in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from lithops) (6.0)\n",
      "Requirement already satisfied: tqdm in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from lithops) (4.62.3)\n",
      "Requirement already satisfied: kubernetes in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from lithops) (21.7.0)\n",
      "Requirement already satisfied: ibm-vpc in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from lithops) (0.9.0)\n",
      "Requirement already satisfied: lxml in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from lithops) (4.7.1)\n",
      "Requirement already satisfied: docker in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from lithops) (5.0.3)\n",
      "Requirement already satisfied: ibm-cos-sdk in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from lithops) (2.11.0)\n",
      "Requirement already satisfied: tblib in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from lithops) (1.7.0)\n",
      "Requirement already satisfied: websocket-client>=0.32.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from docker->lithops) (1.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests->lithops) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests->lithops) (2.0.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests->lithops) (1.26.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests->lithops) (3.3)\n",
      "Requirement already satisfied: ibm-cos-sdk-core==2.11.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from ibm-cos-sdk->lithops) (2.11.0)\n",
      "Requirement already satisfied: ibm-cos-sdk-s3transfer==2.11.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from ibm-cos-sdk->lithops) (2.11.0)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from ibm-cos-sdk->lithops) (0.10.0)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from python-dateutil->lithops) (1.16.0)\n",
      "Requirement already satisfied: ibm-cloud-sdk-core>=3.13.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from ibm-vpc->lithops) (3.13.2)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from kubernetes->lithops) (2.3.3)\n",
      "Requirement already satisfied: setuptools>=21.0.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from kubernetes->lithops) (58.3.0)\n",
      "Requirement already satisfied: requests-oauthlib in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from kubernetes->lithops) (1.3.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib->lithops) (8.4.0)\n",
      "Requirement already satisfied: numpy>=1.16 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib->lithops) (1.21.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib->lithops) (0.11.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib->lithops) (2.4.7)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib->lithops) (1.3.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pandas->lithops) (2021.3)\n",
      "Requirement already satisfied: cryptography>=2.5 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from paramiko->lithops) (36.0.1)\n",
      "Requirement already satisfied: bcrypt>=3.1.3 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from paramiko->lithops) (3.2.0)\n",
      "Requirement already satisfied: pynacl>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from paramiko->lithops) (1.4.0)\n",
      "Requirement already satisfied: deprecated>=1.2.3 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from redis->lithops) (1.2.13)\n",
      "Requirement already satisfied: packaging>=21.3 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from redis->lithops) (21.3)\n",
      "Requirement already satisfied: scipy>=1.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from seaborn->lithops) (1.7.2)\n",
      "Requirement already satisfied: cffi>=1.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from bcrypt>=3.1.3->paramiko->lithops) (1.15.0)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from deprecated>=1.2.3->redis->lithops) (1.13.3)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from google-auth>=1.0.1->kubernetes->lithops) (4.8)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from google-auth>=1.0.1->kubernetes->lithops) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from google-auth>=1.0.1->kubernetes->lithops) (4.2.4)\n",
      "Requirement already satisfied: PyJWT<3.0.0,>=2.0.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from ibm-cloud-sdk-core>=3.13.2->ibm-vpc->lithops) (2.3.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests-oauthlib->kubernetes->lithops) (3.1.1)\n",
      "Requirement already satisfied: pycparser in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from cffi>=1.1->bcrypt>=3.1.3->paramiko->lithops) (2.20)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes->lithops) (0.4.8)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(f'\\nInstall joblib\\n')\n",
    "!{sys.executable} -m pip install joblib\n",
    "#!pip install joblib\n",
    "print(f'\\nInstall lithops\\n')\n",
    "!{sys.executable} -m pip install lithops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SQL\n",
    "import sqlite3\n",
    "\n",
    "# ML\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "\n",
    "# general\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import warnings\n",
    "import string \n",
    "import glob\n",
    "from shutil import copyfile\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from functools import partial\n",
    "\n",
    "# visual\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# notebook\n",
    "from IPython.display import display\n",
    "\n",
    "# threading\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# lithops\n",
    "import lithops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.8.5\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hide Warnings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Disable Autoscrolling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
       "    return false;\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Set Random Seed**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Assisting Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_configurations():\n",
    "    # Set the value options\n",
    "    global max_rows, csv_columns, db_file_name, csv_file_name, columns_type\n",
    "    global csv_index, csv_ending, amount_of_files\n",
    "    global map_reduce_folder_names, amount_of_process, map_regex, db_columns\n",
    "    global db_columns_type, db_table_name, reduce_regex_init, reduce_regex_final\n",
    "    \n",
    "    \n",
    "    Current_python_file_path = os.getcwd()\n",
    "    max_rows = 10\n",
    "    amount_of_files = 20\n",
    "    amount_of_process = 2\n",
    "    csv_columns = ['firstname','secondname','city']\n",
    "    db_columns = ['key', 'value']\n",
    "    map_reduce_folder_names =  [os.path.join(Current_python_file_path, 'mapreducetemp'), \n",
    "                                os.path.join(Current_python_file_path, 'mapreducefinal')]\n",
    "\n",
    "    db_columns_type = [ 'text',  'text']\n",
    "\n",
    "    db_file_name = 'mydata.db'\n",
    "    csv_file_name = 'myCSV'\n",
    "    csv_ending = '.csv'\n",
    "    map_regex = 'part-tmp-'\n",
    "    reduce_regex_init = 'part-'\n",
    "    reduce_regex_final = '-final'\n",
    "\n",
    "    db_table_name = 'temp_results'\n",
    "\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set relevant configurations\n",
    "init_configurations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>\n",
    "# Question 1\n",
    "# Initial Steps\n",
    "\n",
    "Write Python code to create 20 different CSV files in this format:  `myCSV[Number].csv`, where each file contains 10 records. \n",
    "\n",
    "The schema is `(‘firstname’,’secondname’,city’)`  \n",
    "\n",
    "Values should be randomly chosen from the lists: \n",
    "- `firstname` : `[John, Dana, Scott, Marc, Steven, Michael, Albert, Johanna]`  \n",
    "- `city` : `[New York, Haifa, München, London, Palo Alto,  Tel Aviv, Kiel, Hamburg]`  \n",
    "- `secondname`: any value  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class single_record():\n",
    "    firstname = ''\n",
    "    secoundname = ''\n",
    "    city = ''\n",
    "    \n",
    "    # Set the value options\n",
    "    firstname_option_list  = ['John', 'Dana', 'Scott', 'Marc', 'Steven', 'Michael', 'Albert', 'Johanna']\n",
    "    secoundname_option_list  = ['John', 'Dana', 'Scott', 'Marc', 'Steven', 'Michael', 'Albert', 'Johanna']\n",
    "    city_option_list = ['NewYork', 'Haifa', 'Munchen', 'London', 'PaloAlto',  'TelAviv', 'Kiel', 'Hamburg']\n",
    "    def __init__(self, id):\n",
    "        self.firstname = random.choice(self.firstname_option_list)\t\n",
    "        random_name  = ''.join(random.choice(string.ascii_uppercase + string.ascii_lowercase) for _ in range(random.randint(2,4)))\n",
    "        self.secoundname = random_name\n",
    "        self.city = random.choice(self.city_option_list)\n",
    "\n",
    "def create_csvdatabase_file(max_rows):\n",
    "    # this loop generate single fruit\n",
    "    create_n_list_in_advance = max_rows*[None]\n",
    "    for i_row_index in range(max_rows):\n",
    "        i_record = single_record(i_row_index)\n",
    "        create_n_list_in_advance[i_row_index] = [i_record.firstname, i_record.secoundname, i_record.city]\n",
    "    \n",
    "    mydata_df = pd.DataFrame(create_n_list_in_advance, columns = csv_columns)\n",
    "    #mydata_df.to_csv(csv_file_name)  #TODO verify ROY commented this\n",
    "    return mydata_df\n",
    "\n",
    "def generate_n_csv_files():\n",
    "    for i_csv_index in range(0, amount_of_files):\n",
    "        mydata_df = create_csvdatabase_file(max_rows)\n",
    "        mydata_df.to_csv(csv_file_name + str(i_csv_index) + csv_ending)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate all csv files\n",
    "generate_n_csv_files()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use python to Create `mapreducetemp` and `mapreducefinal` folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_new_folder(path):\n",
    "    Path(path).mkdir(parents=True, exist_ok=True)\n",
    "    return \n",
    "\n",
    "def generate_map_reduce_folders():\n",
    "    for i_folder in map_reduce_folder_names:\n",
    "        print(f'Creating folder: {i_folder}')\n",
    "        create_new_folder(i_folder)\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating folder: /Users/royrubin/PycharmProjects/BigDataPlatforms/HW3/mapreducetemp\n",
      "Creating folder: /Users/royrubin/PycharmProjects/BigDataPlatforms/HW3/mapreducefinal\n"
     ]
    }
   ],
   "source": [
    "# generate folders\n",
    "generate_map_reduce_folders()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "# Question 2\n",
    "## MapReduceEngine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Write Python code to create an SQLite database with the following table\n",
    "\n",
    "`TableName: temp_results`   \n",
    "`schema: (key:TEXT,value:TEXT)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_database(db_file_name):\n",
    "    is_exist = os.path.exists(db_file_name)\n",
    "    if  is_exist:\n",
    "        os.remove(db_file_name)\n",
    "    con = sqlite3.connect(db_file_name)\n",
    "        \n",
    "    \n",
    "    cur = con.cursor()\n",
    "    \n",
    "    # Create table\n",
    "    columns_type_list = list(map(lambda x,y: x+' ' + y, db_columns, db_columns_type))\n",
    "    columns_type_list_string = \"(\"+\", \".join(map(str, columns_type_list))+\")\"\n",
    "\n",
    "    cur.execute(''' \n",
    "                CREATE TABLE temp_results\n",
    "                ''' + columns_type_list_string + \\\n",
    "               '''''')\n",
    "    \n",
    "    con.commit()\n",
    "    con.close()\n",
    "    return con, cur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_conn, cur = create_database(db_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Create a Python class** `MapReduceEngine` with method `def execute(input_data, map_function, reduce_function)`, such that:\n",
    "    - `input_data`: is an array of elements\n",
    "    - `map_function`: is a pointer to the Python function that returns a list where each entry of the form (key,value) \n",
    "    - `reduce_function`: is pointer to the Python function that returns a list where each entry of the form (key,value)\n",
    "\n",
    "<br><br>\n",
    "\n",
    "3.**Implement** the following functionality in the `execute(...)` function:\n",
    "\n",
    "<br>\n",
    "\n",
    "1. For each key  from the  input_data, start a new Python thread that executes map_function(key) \n",
    "<br><br>\n",
    "2. Each thread will store results of the map_function into mapreducetemp/part-tmp-X.csv where X is a unique number per each thread.\n",
    "<br><br>\n",
    "3. Keep the list of all threads and check whether they are completed.\n",
    "<br><br>\n",
    "4. Once all threads completed, load content of all CSV files into the temp_results table in SQLite.\n",
    "\n",
    "    Remark: Easiest way to loop over all CSV files and load them into Pandas first, then load into SQLite  \n",
    "    `data = pd.read_csv(path to csv)`  \n",
    "    `data.to_sql(‘temp_results’,sql_conn, if_exists=’append’,index=False)`\n",
    "<br><br>\n",
    "\n",
    "5. **Write SQL statement** that generates a sorted list by key of the form `(key, value)` where value is concatenation of ALL values in the value column that match specific key. For example, if table has records\n",
    "<table>\n",
    "    <tbody>\n",
    "            <tr>\n",
    "                <td style=\"text-align:center\">John</td>\n",
    "                <td style=\"text-align:center\">myCSV1.csv</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td style=\"text-align:center\">Dana</td>\n",
    "                <td style=\"text-align:center\">myCSV5.csv</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td style=\"text-align:center\">John</td>\n",
    "                <td style=\"text-align:center\">myCSV7.csv</td>\n",
    "            </tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "\n",
    "    Then SQL statement will return `(‘John’,’myCSV1.csv, myCSV7.csv’)`  \n",
    "    Remark: use GROUP_CONCAT and also GROUP BY ORDER BY\n",
    "<br><br><br>\n",
    "6. **Start a new thread** for each value from the generated list in the previous step, to execute `reduce_function(key,value)` \n",
    "<br>    \n",
    "7. Each thread will store results of reduce_function into `mapreducefinal/part-X-final.csv` file  \n",
    "<br>\n",
    "8. Keep list of all threads and check whether they are completed  \n",
    "<br>\n",
    "9. Once all threads completed, print on the screen `MapReduce Completed` otherwise print `MapReduce Failed` \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement all of the class here\n",
    "\n",
    "class MapReduceEngine():\n",
    "    def execute(self, input_data, map_function, reduce_function):\n",
    "        #  1) For each key  from the  input_data, start a new Python thread that executes \n",
    "        #     map_function(key)\n",
    "        #  2) Each thread will store results of the map_function into \n",
    "        #     mapreducetemp/part-tmp-X.csv where X\n",
    "        #     is a unique number per each thread.\n",
    "        #  3) Keep the list of all threads and check whether they are completed\n",
    "        \n",
    "        # succeed_new_path_list  = Parallel(n_jobs=amount_of_process, backend=\"threading\", \\\n",
    "        #                                   prefer=\"processes\")(delayed(self._run_at_parallel_map)(\n",
    "        #     index, item, map_function) for index, item in enumerate(input_data))\n",
    "        \n",
    "        \n",
    "        fexec = lithops.FunctionExecutor(config=config)\n",
    "        \n",
    "        \n",
    "        for index, item in enumerate(input_data):\n",
    "            fexec.call_async(self._run_at_parallel_map, index, item, map_function)\n",
    "            \n",
    "        \n",
    "        \n",
    "        # 4) Once all threads completed, load content of all CSV files into the temp_results\n",
    "        #    table in SQLite\n",
    "        \n",
    "#         # get new files names\n",
    "#         filepaths = [path for boolean, path in succeed_new_path_list]\n",
    "        \n",
    "#         # write generated csv files to sql data base\n",
    "#         sql_conn = sqlite3.connect(db_file_name)\n",
    "#         list(map(lambda x: pd.read_csv(x, index_col=0) \\\n",
    "#                  .to_sql('temp_results',sql_conn, if_exists='append',index=False), filepaths))    \n",
    "#         sql_conn.close()\n",
    "        \n",
    "#         # get list of succeed or failed of threads\n",
    "#         boolean_results = [boolean for boolean, path in succeed_new_path_list]\n",
    "        \n",
    "#         # validate that all threads are completed succesfully\n",
    "#         if False in boolean_results:\n",
    "#             status = 'Map Reduce Failed'\n",
    "#             return status\n",
    "\n",
    "#         #print_db_file_info()\n",
    "        \n",
    "#         # 5) **Write SQL statement** that generates a sorted list by key of the form \n",
    "#         #    `(key, value)` where value is concatenation of ALL values in the value column\n",
    "#         #     that match specific key. For example, if table has records\n",
    "        \n",
    "#         # query data base using GROUP_CONCAT and GROUP BY  and ORDER BY \n",
    "#         generated_list = self._get_grouped_info_from_db_by_key(key='key')\n",
    "        \n",
    "#         # 6) **Start a new thread** for each value from\n",
    "#         #    the generated list in the previous step, to execute `reduce_function(key,value)\n",
    "#         #    Begin by Performing REDUCE actions\n",
    "#         #    we will open a thread for each REDUCE\n",
    "#         # 7) Each thread will store results of reduce_function into \n",
    "#         #   `mapreducefinal/part-X-final.csv` file\n",
    "        \n",
    "#         # 8) Keep list of all threads and check whether they are completed\n",
    "        \n",
    "#         reduce_return_dict_succeed  = Parallel(n_jobs=amount_of_process, backend=\"threading\",\\\n",
    "#                                                prefer=\"processes\")(delayed(self._run_at_parallel_reduce)(\n",
    "#             index, item, reduce_function) for index, item in enumerate(generated_list))\n",
    "\n",
    "        \n",
    "#         # 9) Once all threads completed, print on the screen \n",
    "#         #   `MapReduce Completed` otherwise print `MapReduce Failed`\n",
    "#         boolean_results = [boolean for boolean, path in reduce_return_dict_succeed]\n",
    "        \n",
    "#         if False in boolean_results:\n",
    "#             status = 'Map Reduce Failed'\n",
    "#             return status\n",
    "        \n",
    "#         reduce_dict = [reduce_dict for boolean, reduce_dict in reduce_return_dict_succeed]\n",
    "    \n",
    "#         reduce_df = pd.DataFrame(data= reduce_dict)\n",
    "#         status = 'Map Reduce Completed'\n",
    "        \n",
    "        return status\n",
    "\n",
    "    def _run_at_parallel_map(self, i_item_index, i_item, function):\n",
    "        new_path = None\n",
    "        succeed = True\n",
    "        try:\n",
    "\n",
    "            new_path  = os.path.join(map_reduce_folder_names[0],\n",
    "                                    map_regex + str(i_item_index) + csv_ending) \n",
    "\n",
    "            dict_result = function(i_item)\n",
    "            result_df = pd.DataFrame(data=dict_result, columns = ['key', 'value'])\n",
    "\n",
    "            result_df.to_csv(new_path) \n",
    "\n",
    "            succeed = os.path.exists(new_path)\n",
    "        except:\n",
    "            succeed = False\n",
    "\n",
    "        return  succeed, new_path\n",
    "\n",
    "\n",
    "    def _run_at_parallel_reduce(self, i_item_index, i_item, function):\n",
    "        new_path = None\n",
    "        succeed = True\n",
    "        try:\n",
    "            new_path  = os.path.join(map_reduce_folder_names[1],\n",
    "                                     reduce_regex_init + str(i_item_index) + reduce_regex_final + csv_ending)\n",
    "            #print(new_path)\n",
    "            key = i_item[0]\n",
    "            value = i_item[1]\n",
    "\n",
    "            dict_result = function(key, value)\n",
    "            result_df = pd.DataFrame.from_records([dict_result])\n",
    "\n",
    "            result_df.to_csv(new_path) \n",
    "\n",
    "            succeed = os.path.exists(new_path)\n",
    "        except:\n",
    "            succeed = False\n",
    "\n",
    "        return  succeed, new_path\n",
    "\n",
    "\n",
    "    def _get_grouped_info_from_db_by_key(self, key):\n",
    "        con = sqlite3.connect(db_file_name)\n",
    "        cur = con.cursor()\n",
    "        return_list = []\n",
    "        for row in cur.execute('SELECT key, GROUP_CONCAT(value) FROM ' +db_table_name+ ' GROUP BY ' + key + ' ORDER BY ' + key):\n",
    "            #print(row)\n",
    "            return_list.append(row)\n",
    "        con.close()\n",
    "        return return_list\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>\n",
    "\n",
    "# Question 3\n",
    "## Implement the MapReduce Inverted index of the JSON documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a function `inverted_map(document_name)` which reads the CSV document from the local disc and return a list that contains entries of the form (key_value, document name).\n",
    "\n",
    "For example, if myCSV4.csv document has values like:  \n",
    "`{‘firstname’:’John’,‘secondname’:’Rambo’,‘city’:’Palo Alto’}`\n",
    "\n",
    "Then `inverted_map(‘myCSV4.csv’)` function will return a list:  \n",
    "`[(‘firstname_John’,’ myCSV4.csv’),(‘secondname_Rambo’,’ myCSV4.csv’), (‘city_Palo Alto’,’ myCSV4.csv’)]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverted_map(document_name):\n",
    "    #ducument_name = input_data[0]\n",
    "    csv_df = pd.read_csv(document_name, index_col=0)\n",
    "    csv_size = csv_df.shape[0]\n",
    "    csv_columns = csv_df.columns.to_list()\n",
    "    output_list = []\n",
    "    for i_col in csv_columns:\n",
    "        col_vals  = csv_df[i_col].to_list()\n",
    "        curr_ouput = list(map(lambda x,y,z: (x+ '_' + y, z) , \\\n",
    "                              csv_size*[i_col], col_vals, csv_size*[document_name]))\n",
    "        output_list += curr_ouput\n",
    "    \n",
    "    return output_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a reduce function `inverted_reduce(value, documents)`, where the field “documents” contains a list of all CSV documents per given value.   \n",
    "This list might have duplicates.   \n",
    "Reduce function will return new list without duplicates.\n",
    "\n",
    "For example,  \n",
    "calling the function `inverted_reduce(‘firstname_Albert’,’myCSV2.csv, myCSV5.csv,myCSV2.csv’)`   \n",
    "will return a list `[‘firstname_Albert’,’myCSV2.csv, myCSV5.csv,myCSV2.csv’]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverted_reduce(value, documents):\n",
    "    ducument_name_list = documents.split(',')\n",
    "    ducument_name_list_no_duplicates = list(set(ducument_name_list))\n",
    "    string_ducument_name_list_no_duplicates = (', ').join(ducument_name_list_no_duplicates)\n",
    "    return_list = [value, string_ducument_name_list_no_duplicates]\n",
    "    return return_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>\n",
    "# Question 4\n",
    "## Testing Your MapReduce\n",
    "\n",
    "**Create Python list** `input_data` : `[‘myCSV1.csv’,.. ,‘myCSV20.csv’]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_data:\n",
      "['myCSV1.csv', 'myCSV0.csv', 'myCSV2.csv', 'myCSV3.csv', 'myCSV7.csv', 'myCSV6.csv', 'myCSV4.csv', 'myCSV5.csv', 'myCSV19.csv', 'myCSV18.csv', 'myCSV10.csv', 'myCSV11.csv', 'myCSV13.csv', 'myCSV12.csv', 'myCSV16.csv', 'myCSV17.csv', 'myCSV15.csv', 'myCSV14.csv', 'myCSV8.csv', 'myCSV9.csv']\n"
     ]
    }
   ],
   "source": [
    "input_data = glob.glob(csv_file_name+'*'+csv_ending)\n",
    "print(f'input_data:\\n{input_data}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Submit MapReduce as follows:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-07 14:52:29,243 [INFO] lithops.config -- Lithops v2.5.8\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Unsupported Python version: 3.10",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/lithops/serverless/backends/ibm_cf/config.py\u001b[0m in \u001b[0;36mload_config\u001b[0;34m(config_data)\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0mconfig_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ibm_cf'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'runtime'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRUNTIME_DEFAULT\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpython_version\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: '3.10'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/0f/27_6jz_d76qdtz0_10p1wjxr0000gp/T/ipykernel_2333/2621779248.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmapreduce\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMapReduceEngine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mstatus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmapreduce\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minverted_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minverted_reduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/0f/27_6jz_d76qdtz0_10p1wjxr0000gp/T/ipykernel_2333/200347441.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, input_data, map_function, reduce_function)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mfexec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlithops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFunctionExecutor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/lithops/executors.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, mode, config, backend, storage, runtime, runtime_memory, monitoring, max_workers, worker_processes, remote_invoker, log_level)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;31m# Load configuration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefault_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig_ow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_cleaner\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lithops'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data_cleaner'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/lithops/config.py\u001b[0m in \u001b[0;36mdefault_config\u001b[0;34m(config_data, config_overwrite, load_storage_config)\u001b[0m\n\u001b[1;32m    185\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loading Serverless backend module: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0mcb_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'lithops.serverless.backends.{}.config'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcb_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0mverify_runtime_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'runtime'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/lithops/serverless/backends/ibm_cf/config.py\u001b[0m in \u001b[0;36mload_config\u001b[0;34m(config_data)\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0mconfig_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ibm_cf'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'runtime'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRUNTIME_DEFAULT\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpython_version\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unsupported Python version: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_version\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mException\u001b[0m: Unsupported Python version: 3.10"
     ]
    }
   ],
   "source": [
    "mapreduce = MapReduceEngine()\n",
    "status = mapreduce.execute(input_data, inverted_map, inverted_reduce)\n",
    "print(status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure that `MapReduce Completed` should be printed and `mapreducefinal` folder should contain the result files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use python to delete all temporary data from mapreducetemp folder and delete SQLite database:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# remove temporary folders & files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete all csv files\n",
    "for i_file in input_data:\n",
    "    if os.path.exists(i_file):\n",
    "        os.remove(i_file)\n",
    "if os.path.exists(db_file_name):\n",
    "    os.remove(db_file_name)\n",
    "for i_folder in map_reduce_folder_names:\n",
    "    if os.path.exists(i_folder):\n",
    "        shutil.rmtree(i_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>\n",
    "\n",
    "# Question 5\n",
    "# Final Thoughts\n",
    "\n",
    "The phase where `MapReduceEngine` reads all temporary files generated by maps and sort them to provide each reducer a specific key is called the **shuffle step**.\n",
    "\n",
    "Please explain **clearly** what would be the main problem of MapReduce when processing Big Data, if there is no shuffle step at all, meaning reducers will directly read responses from the mappers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The main problem of MapReduce if there is no shuffle step is:\n",
    "\n",
    "> **the reduce step will not be performed as expected (we could say it failed)**.\n",
    "\n",
    "> See image below (from the lecture):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://1.bp.blogspot.com/-UvgLSDv7Rb4/Tbpn3veAOTI/AAAAAAAAAVk/kdaMzLa50BE/s1600/WordCountFlow.JPG\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "We can see in the example diagram above that if a reduce worker will recieve the output of the map worker directly, then the reduce function will return the wrong output.\n",
    "<br>\n",
    "for example:\n",
    "the bottom map worker will output:\n",
    "\n",
    " - apple,1\n",
    " \n",
    " - apple,1\n",
    " \n",
    " - plum,1\n",
    " \n",
    "the bottom reduce worker will recieve this as input, and might output:\n",
    "\n",
    " - apple,2\n",
    " \n",
    " - plum,1\n",
    " \n",
    "<br>\n",
    "After the reduce worker finished there is no other reduce step.\n",
    "<br>\n",
    "And so, If we will do the same for all map and reduce workers, we will get the following WRONG unreduced and unsummed output:\n",
    "\n",
    " - apple,1\n",
    " \n",
    " - orange,1\n",
    " \n",
    " - mango,1\n",
    " \n",
    " - orange,1\n",
    " \n",
    " - graspes,1\n",
    " \n",
    " - plum,1\n",
    " \n",
    " - apple,1\n",
    " \n",
    " - plum,1\n",
    " \n",
    " - mango,1\n",
    " \n",
    " - apple,2\n",
    " \n",
    " - plum,1\n",
    " \n",
    "<br>\n",
    "which is obviously not the wanted output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>\n",
    "Thank You :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
