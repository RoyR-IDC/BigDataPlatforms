{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big Data Platform\n",
    "## Assignment 2: MapReduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**By:**  \n",
    "\n",
    "Or Livne, 300123123  \n",
    "Roy Rubin, 201312907\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The goal of this assignment is to:**\n",
    "- Understand and practice the details of MapReduceEngine\n",
    "\n",
    "**Instructions:**\n",
    "- Students will form teams of two people each, and submit a single homework for each team.\n",
    "- The same score for the homework will be given to each member of your team.\n",
    "- Your solution is in the form of a Jupyter notebook file (with extension ipynb).\n",
    "- Images/Graphs/Tables should be submitted inside the notebook.\n",
    "- The notebook should be runnable and properly documented. \n",
    "- Please answer all the questions and include all your code.\n",
    "- You are expected to submit a clear and pythonic code.\n",
    "- You can change functions signatures/definitions.\n",
    "\n",
    "**Submission:**\n",
    "- Submission of the homework will be done via Moodle by uploading a Jupyter notebook.\n",
    "- The homework needs to be entirely in English.\n",
    "- The deadline for submission is on Moodle.\n",
    "- Late submission won't be allowed.\n",
    "  \n",
    "  \n",
    "- In case of identical code submissions - both groups will get a Zero. \n",
    "- Some groups might be selected randomly to present their code.\n",
    "\n",
    "**Requirements:**  \n",
    "- Python 3.6 should be used.  \n",
    "- You should implement the algorithms by yourself using only basic Python libraries (such as numpy,pandas,etc.)\n",
    "\n",
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Grading:**\n",
    "- Q1 - 5 points - Initial Steps\n",
    "- Q2 - 50 points - MapReduceEngine\n",
    "- Q3 - 30 points - Implement the MapReduce Inverted index of the JSON documents\n",
    "- Q4 - 5 points - Testing Your MapReduce\n",
    "- Q5 - 10 points - Final Thoughts \n",
    "\n",
    "`Total: 100`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prerequisites**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Install joblib\n",
      "\n",
      "\n",
      "Install lithops\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'C:\\Users\\or' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'C:\\Users\\or' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(f'\\nInstall joblib\\n')\n",
    "!{sys.executable} -m pip install joblib\n",
    "#!pip install joblib\n",
    "print(f'\\nInstall lithops\\n')\n",
    "!{sys.executable} -m pip install lithops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SQL\n",
    "import sqlite3\n",
    "\n",
    "# ML\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "\n",
    "# general\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import warnings\n",
    "import string \n",
    "import glob\n",
    "from shutil import copyfile\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from functools import partial\n",
    "\n",
    "# visual\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# notebook\n",
    "from IPython.display import display\n",
    "\n",
    "# threading\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# lithops\n",
    "import lithops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.7.11\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hide Warnings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Disable Autoscrolling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
       "    return false;\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Set Random Seed**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Assisting Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_configurations():\n",
    "    # Set the value options\n",
    "    global max_rows, csv_columns, db_file_name, csv_file_name, columns_type\n",
    "    global csv_index, csv_ending, amount_of_files\n",
    "    global map_reduce_folder_names, amount_of_process, map_regex, db_columns\n",
    "    global db_columns_type, db_table_name, reduce_regex_init, reduce_regex_final\n",
    "    \n",
    "    \n",
    "    Current_python_file_path = os.getcwd()\n",
    "    max_rows = 10\n",
    "    amount_of_files = 20\n",
    "    amount_of_process = 2\n",
    "    csv_columns = ['firstname','secondname','city']\n",
    "    db_columns = ['key', 'value']\n",
    "    map_reduce_folder_names =  [os.path.join(Current_python_file_path, 'mapreducetemp'), \n",
    "                                os.path.join(Current_python_file_path, 'mapreducefinal')]\n",
    "\n",
    "    db_columns_type = [ 'text',  'text']\n",
    "\n",
    "    db_file_name = 'mydata.db'\n",
    "    csv_file_name = 'myCSV'\n",
    "    csv_ending = '.csv'\n",
    "    map_regex = 'part-tmp-'\n",
    "    reduce_regex_init = 'part-'\n",
    "    reduce_regex_final = '-final'\n",
    "\n",
    "    db_table_name = 'temp_results'\n",
    "\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set relevant configurations\n",
    "init_configurations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>\n",
    "# Question 1\n",
    "# Initial Steps\n",
    "\n",
    "Write Python code to create 20 different CSV files in this format:  `myCSV[Number].csv`, where each file contains 10 records. \n",
    "\n",
    "The schema is `(‘firstname’,’secondname’,city’)`  \n",
    "\n",
    "Values should be randomly chosen from the lists: \n",
    "- `firstname` : `[John, Dana, Scott, Marc, Steven, Michael, Albert, Johanna]`  \n",
    "- `city` : `[New York, Haifa, München, London, Palo Alto,  Tel Aviv, Kiel, Hamburg]`  \n",
    "- `secondname`: any value  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class single_record():\n",
    "    firstname = ''\n",
    "    secoundname = ''\n",
    "    city = ''\n",
    "    \n",
    "    # Set the value options\n",
    "    firstname_option_list  = ['John', 'Dana', 'Scott', 'Marc', 'Steven', 'Michael', 'Albert', 'Johanna']\n",
    "    secoundname_option_list  = ['John', 'Dana', 'Scott', 'Marc', 'Steven', 'Michael', 'Albert', 'Johanna']\n",
    "    city_option_list = ['NewYork', 'Haifa', 'Munchen', 'London', 'PaloAlto',  'TelAviv', 'Kiel', 'Hamburg']\n",
    "    def __init__(self, id):\n",
    "        self.firstname = random.choice(self.firstname_option_list)\t\n",
    "        random_name  = ''.join(random.choice(string.ascii_uppercase + string.ascii_lowercase) for _ in range(random.randint(2,4)))\n",
    "        self.secoundname = random_name\n",
    "        self.city = random.choice(self.city_option_list)\n",
    "\n",
    "def create_csvdatabase_file(max_rows):\n",
    "    # this loop generate single fruit\n",
    "    create_n_list_in_advance = max_rows*[None]\n",
    "    for i_row_index in range(max_rows):\n",
    "        i_record = single_record(i_row_index)\n",
    "        create_n_list_in_advance[i_row_index] = [i_record.firstname, i_record.secoundname, i_record.city]\n",
    "    \n",
    "    mydata_df = pd.DataFrame(create_n_list_in_advance, columns = csv_columns)\n",
    "    #mydata_df.to_csv(csv_file_name)  #TODO verify ROY commented this\n",
    "    return mydata_df\n",
    "\n",
    "def generate_n_csv_files():\n",
    "    for i_csv_index in range(0, amount_of_files):\n",
    "        mydata_df = create_csvdatabase_file(max_rows)\n",
    "        mydata_df.to_csv(csv_file_name + str(i_csv_index) + csv_ending)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate all csv files\n",
    "generate_n_csv_files()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use python to Create `mapreducetemp` and `mapreducefinal` folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_new_folder(path):\n",
    "    Path(path).mkdir(parents=True, exist_ok=True)\n",
    "    return \n",
    "\n",
    "def generate_map_reduce_folders():\n",
    "    for i_folder in map_reduce_folder_names:\n",
    "        print(f'Creating folder: {i_folder}')\n",
    "        create_new_folder(i_folder)\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating folder: C:\\Msc\\Git\\big\\HW3\\mapreducetemp\n",
      "Creating folder: C:\\Msc\\Git\\big\\HW3\\mapreducefinal\n"
     ]
    }
   ],
   "source": [
    "# generate folders\n",
    "generate_map_reduce_folders()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "# Question 2\n",
    "## MapReduceEngine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Write Python code to create an SQLite database with the following table\n",
    "\n",
    "`TableName: temp_results`   \n",
    "`schema: (key:TEXT,value:TEXT)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_database(db_file_name):\n",
    "    is_exist = os.path.exists(db_file_name)\n",
    "    if  is_exist:\n",
    "        os.remove(db_file_name)\n",
    "    con = sqlite3.connect(db_file_name)\n",
    "        \n",
    "    \n",
    "    cur = con.cursor()\n",
    "    \n",
    "    # Create table\n",
    "    columns_type_list = list(map(lambda x,y: x+' ' + y, db_columns, db_columns_type))\n",
    "    columns_type_list_string = \"(\"+\", \".join(map(str, columns_type_list))+\")\"\n",
    "\n",
    "    cur.execute(''' \n",
    "                CREATE TABLE temp_results\n",
    "                ''' + columns_type_list_string + \\\n",
    "               '''''')\n",
    "    \n",
    "    con.commit()\n",
    "    con.close()\n",
    "    return con, cur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_conn, cur = create_database(db_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Create a Python class** `MapReduceEngine` with method `def execute(input_data, map_function, reduce_function)`, such that:\n",
    "    - `input_data`: is an array of elements\n",
    "    - `map_function`: is a pointer to the Python function that returns a list where each entry of the form (key,value) \n",
    "    - `reduce_function`: is pointer to the Python function that returns a list where each entry of the form (key,value)\n",
    "\n",
    "<br><br>\n",
    "\n",
    "3.**Implement** the following functionality in the `execute(...)` function:\n",
    "\n",
    "<br>\n",
    "\n",
    "1. For each key  from the  input_data, start a new Python thread that executes map_function(key) \n",
    "<br><br>\n",
    "2. Each thread will store results of the map_function into mapreducetemp/part-tmp-X.csv where X is a unique number per each thread.\n",
    "<br><br>\n",
    "3. Keep the list of all threads and check whether they are completed.\n",
    "<br><br>\n",
    "4. Once all threads completed, load content of all CSV files into the temp_results table in SQLite.\n",
    "\n",
    "    Remark: Easiest way to loop over all CSV files and load them into Pandas first, then load into SQLite  \n",
    "    `data = pd.read_csv(path to csv)`  \n",
    "    `data.to_sql(‘temp_results’,sql_conn, if_exists=’append’,index=False)`\n",
    "<br><br>\n",
    "\n",
    "5. **Write SQL statement** that generates a sorted list by key of the form `(key, value)` where value is concatenation of ALL values in the value column that match specific key. For example, if table has records\n",
    "<table>\n",
    "    <tbody>\n",
    "            <tr>\n",
    "                <td style=\"text-align:center\">John</td>\n",
    "                <td style=\"text-align:center\">myCSV1.csv</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td style=\"text-align:center\">Dana</td>\n",
    "                <td style=\"text-align:center\">myCSV5.csv</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td style=\"text-align:center\">John</td>\n",
    "                <td style=\"text-align:center\">myCSV7.csv</td>\n",
    "            </tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "\n",
    "    Then SQL statement will return `(‘John’,’myCSV1.csv, myCSV7.csv’)`  \n",
    "    Remark: use GROUP_CONCAT and also GROUP BY ORDER BY\n",
    "<br><br><br>\n",
    "6. **Start a new thread** for each value from the generated list in the previous step, to execute `reduce_function(key,value)` \n",
    "<br>    \n",
    "7. Each thread will store results of reduce_function into `mapreducefinal/part-X-final.csv` file  \n",
    "<br>\n",
    "8. Keep list of all threads and check whether they are completed  \n",
    "<br>\n",
    "9. Once all threads completed, print on the screen `MapReduce Completed` otherwise print `MapReduce Failed` \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {'lithops': {'backend': 'ibm_cf', 'storage': 'ibm_cos'},\n",
    "\n",
    "          'ibm_cf': {'endpoint': 'https://us-south.functions.cloud.ibm.com',\n",
    "                     'namespace': 'roy.rubin@post.idc.ac.il_dev',\n",
    "                     'api_key': '1defbac0-eea1-4bb8-b5d4-cee7e63b3bb4:63a0ls32DAGjVe0TkdUcBqcOL7lOtR7bLsQYf98WGgW2xpp9Bpd0BUSubnlsfNQM',\n",
    "                     },\n",
    "          'ibm_cos': {'storage_bucket': 'cloud-object-storage-mq-cos-standard-8s4',\n",
    "                      'region': 'eu-de',\n",
    "                      \"access_key\": \"bb21b4d35ef046d19b4f6fd93f39a3a5\",\n",
    "                      \"secret_key\": \"d9db9b67564ec2fc5467820ef8719533cdc2e64a5ce33695\"\n",
    "                      }\n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "def get_running_folder(Input):\n",
    "    current_python_file_path = os.getcwd()\n",
    "    print('--------')\n",
    "    print(current_python_file_path)\n",
    "    print('--------')\n",
    "\n",
    "    return current_python_file_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-07 15:40:09,668 [INFO] lithops.config -- Lithops v2.5.8\n",
      "2022-01-07 15:40:09,680 [INFO] lithops.storage.backends.ibm_cos.ibm_cos -- IBM COS client created - Region: eu-de\n",
      "2022-01-07 15:40:09,682 [INFO] lithops.serverless.backends.ibm_cf.ibm_cf -- IBM CF client created - Region: us-south - Namespace: roy.rubin@post.idc.ac.il_dev\n",
      "2022-01-07 15:40:09,683 [INFO] lithops.invokers -- ExecutorID aa2599-11 | JobID A000 - Selected Runtime: lithopscloud/ibmcf-python-v37 - 256MB\n",
      "2022-01-07 15:40:10,437 [INFO] lithops.invokers -- ExecutorID aa2599-11 | JobID A000 - Starting function invocation: get_running_folder() - Total: 1 activations\n",
      "2022-01-07 15:40:10,441 [INFO] lithops.invokers -- ExecutorID aa2599-11 | JobID A000 - View execution logs at C:\\Users\\or livne\\AppData\\Local\\Temp\\lithops\\logs\\aa2599-11-A000.log\n"
     ]
    }
   ],
   "source": [
    "fexec = lithops.FunctionExecutor(config=config)        \n",
    "t= fexec.call_async(get_running_folder, '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<bound method ResponseFuture.result of <lithops.future.ResponseFuture object at 0x0000026AE021D8C8>>'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(t.result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement all of the class here\n",
    "\n",
    "class MapReduceEngine():\n",
    "    def execute(self, input_data, map_function, reduce_function):\n",
    "        #  1) For each key  from the  input_data, start a new Python thread that executes \n",
    "        #     map_function(key)\n",
    "        #  2) Each thread will store results of the map_function into \n",
    "        #     mapreducetemp/part-tmp-X.csv where X\n",
    "        #     is a unique number per each thread.\n",
    "        #  3) Keep the list of all threads and check whether they are completed\n",
    "        \n",
    "        # succeed_new_path_list  = Parallel(n_jobs=amount_of_process, backend=\"threading\", \\\n",
    "        #                                   prefer=\"processes\")(delayed(self._run_at_parallel_map)(\n",
    "        #     index, item, map_function) for index, item in enumerate(input_data))\n",
    "        \n",
    "        \n",
    "        fexec = lithops.FunctionExecutor(config=config)\n",
    "        \n",
    "        \n",
    "        for index, item in enumerate(input_data):\n",
    "            fexec.call_async(self._run_at_parallel_map, index, item, map_function)\n",
    "            \n",
    "        \n",
    "        \n",
    "        # 4) Once all threads completed, load content of all CSV files into the temp_results\n",
    "        #    table in SQLite\n",
    "        \n",
    "#         # get new files names\n",
    "#         filepaths = [path for boolean, path in succeed_new_path_list]\n",
    "        \n",
    "#         # write generated csv files to sql data base\n",
    "#         sql_conn = sqlite3.connect(db_file_name)\n",
    "#         list(map(lambda x: pd.read_csv(x, index_col=0) \\\n",
    "#                  .to_sql('temp_results',sql_conn, if_exists='append',index=False), filepaths))    \n",
    "#         sql_conn.close()\n",
    "        \n",
    "#         # get list of succeed or failed of threads\n",
    "#         boolean_results = [boolean for boolean, path in succeed_new_path_list]\n",
    "        \n",
    "#         # validate that all threads are completed succesfully\n",
    "#         if False in boolean_results:\n",
    "#             status = 'Map Reduce Failed'\n",
    "#             return status\n",
    "\n",
    "#         #print_db_file_info()\n",
    "        \n",
    "#         # 5) **Write SQL statement** that generates a sorted list by key of the form \n",
    "#         #    `(key, value)` where value is concatenation of ALL values in the value column\n",
    "#         #     that match specific key. For example, if table has records\n",
    "        \n",
    "#         # query data base using GROUP_CONCAT and GROUP BY  and ORDER BY \n",
    "#         generated_list = self._get_grouped_info_from_db_by_key(key='key')\n",
    "        \n",
    "#         # 6) **Start a new thread** for each value from\n",
    "#         #    the generated list in the previous step, to execute `reduce_function(key,value)\n",
    "#         #    Begin by Performing REDUCE actions\n",
    "#         #    we will open a thread for each REDUCE\n",
    "#         # 7) Each thread will store results of reduce_function into \n",
    "#         #   `mapreducefinal/part-X-final.csv` file\n",
    "        \n",
    "#         # 8) Keep list of all threads and check whether they are completed\n",
    "        \n",
    "#         reduce_return_dict_succeed  = Parallel(n_jobs=amount_of_process, backend=\"threading\",\\\n",
    "#                                                prefer=\"processes\")(delayed(self._run_at_parallel_reduce)(\n",
    "#             index, item, reduce_function) for index, item in enumerate(generated_list))\n",
    "\n",
    "        \n",
    "#         # 9) Once all threads completed, print on the screen \n",
    "#         #   `MapReduce Completed` otherwise print `MapReduce Failed`\n",
    "#         boolean_results = [boolean for boolean, path in reduce_return_dict_succeed]\n",
    "        \n",
    "#         if False in boolean_results:\n",
    "#             status = 'Map Reduce Failed'\n",
    "#             return status\n",
    "        \n",
    "#         reduce_dict = [reduce_dict for boolean, reduce_dict in reduce_return_dict_succeed]\n",
    "    \n",
    "#         reduce_df = pd.DataFrame(data= reduce_dict)\n",
    "#         status = 'Map Reduce Completed'\n",
    "        \n",
    "        return status\n",
    "\n",
    "    def _run_at_parallel_map(self, i_item_index, i_item, function):\n",
    "        new_path = None\n",
    "        succeed = True\n",
    "        try:\n",
    "\n",
    "            new_path  = os.path.join(map_reduce_folder_names[0],\n",
    "                                    map_regex + str(i_item_index) + csv_ending) \n",
    "\n",
    "            dict_result = function(i_item)\n",
    "            result_df = pd.DataFrame(data=dict_result, columns = ['key', 'value'])\n",
    "\n",
    "            result_df.to_csv(new_path) \n",
    "\n",
    "            succeed = os.path.exists(new_path)\n",
    "        except:\n",
    "            succeed = False\n",
    "\n",
    "        return  succeed, new_path\n",
    "\n",
    "\n",
    "    def _run_at_parallel_reduce(self, i_item_index, i_item, function):\n",
    "        new_path = None\n",
    "        succeed = True\n",
    "        try:\n",
    "            new_path  = os.path.join(map_reduce_folder_names[1],\n",
    "                                     reduce_regex_init + str(i_item_index) + reduce_regex_final + csv_ending)\n",
    "            #print(new_path)\n",
    "            key = i_item[0]\n",
    "            value = i_item[1]\n",
    "\n",
    "            dict_result = function(key, value)\n",
    "            result_df = pd.DataFrame.from_records([dict_result])\n",
    "\n",
    "            result_df.to_csv(new_path) \n",
    "\n",
    "            succeed = os.path.exists(new_path)\n",
    "        except:\n",
    "            succeed = False\n",
    "\n",
    "        return  succeed, new_path\n",
    "\n",
    "\n",
    "    def _get_grouped_info_from_db_by_key(self, key):\n",
    "        con = sqlite3.connect(db_file_name)\n",
    "        cur = con.cursor()\n",
    "        return_list = []\n",
    "        for row in cur.execute('SELECT key, GROUP_CONCAT(value) FROM ' +db_table_name+ ' GROUP BY ' + key + ' ORDER BY ' + key):\n",
    "            #print(row)\n",
    "            return_list.append(row)\n",
    "        con.close()\n",
    "        return return_list\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>\n",
    "\n",
    "# Question 3\n",
    "## Implement the MapReduce Inverted index of the JSON documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a function `inverted_map(document_name)` which reads the CSV document from the local disc and return a list that contains entries of the form (key_value, document name).\n",
    "\n",
    "For example, if myCSV4.csv document has values like:  \n",
    "`{‘firstname’:’John’,‘secondname’:’Rambo’,‘city’:’Palo Alto’}`\n",
    "\n",
    "Then `inverted_map(‘myCSV4.csv’)` function will return a list:  \n",
    "`[(‘firstname_John’,’ myCSV4.csv’),(‘secondname_Rambo’,’ myCSV4.csv’), (‘city_Palo Alto’,’ myCSV4.csv’)]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverted_map(document_name):\n",
    "    #ducument_name = input_data[0]\n",
    "    csv_df = pd.read_csv(document_name, index_col=0)\n",
    "    csv_size = csv_df.shape[0]\n",
    "    csv_columns = csv_df.columns.to_list()\n",
    "    output_list = []\n",
    "    for i_col in csv_columns:\n",
    "        col_vals  = csv_df[i_col].to_list()\n",
    "        curr_ouput = list(map(lambda x,y,z: (x+ '_' + y, z) , \\\n",
    "                              csv_size*[i_col], col_vals, csv_size*[document_name]))\n",
    "        output_list += curr_ouput\n",
    "    \n",
    "    return output_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a reduce function `inverted_reduce(value, documents)`, where the field “documents” contains a list of all CSV documents per given value.   \n",
    "This list might have duplicates.   \n",
    "Reduce function will return new list without duplicates.\n",
    "\n",
    "For example,  \n",
    "calling the function `inverted_reduce(‘firstname_Albert’,’myCSV2.csv, myCSV5.csv,myCSV2.csv’)`   \n",
    "will return a list `[‘firstname_Albert’,’myCSV2.csv, myCSV5.csv,myCSV2.csv’]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverted_reduce(value, documents):\n",
    "    ducument_name_list = documents.split(',')\n",
    "    ducument_name_list_no_duplicates = list(set(ducument_name_list))\n",
    "    string_ducument_name_list_no_duplicates = (', ').join(ducument_name_list_no_duplicates)\n",
    "    return_list = [value, string_ducument_name_list_no_duplicates]\n",
    "    return return_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>\n",
    "# Question 4\n",
    "## Testing Your MapReduce\n",
    "\n",
    "**Create Python list** `input_data` : `[‘myCSV1.csv’,.. ,‘myCSV20.csv’]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_data:\n",
      "['myCSV0.csv', 'myCSV1.csv', 'myCSV10.csv', 'myCSV11.csv', 'myCSV12.csv', 'myCSV13.csv', 'myCSV14.csv', 'myCSV15.csv', 'myCSV16.csv', 'myCSV17.csv', 'myCSV18.csv', 'myCSV19.csv', 'myCSV2.csv', 'myCSV3.csv', 'myCSV4.csv', 'myCSV5.csv', 'myCSV6.csv', 'myCSV7.csv', 'myCSV8.csv', 'myCSV9.csv']\n"
     ]
    }
   ],
   "source": [
    "input_data = glob.glob(csv_file_name+'*'+csv_ending)\n",
    "print(f'input_data:\\n{input_data}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "def a():\n",
    "    print(listdir())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Submit MapReduce as follows:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-07 15:27:11,882 [INFO] lithops.config -- Lithops v2.5.8\n",
      "2022-01-07 15:27:11,889 [INFO] lithops.storage.backends.ibm_cos.ibm_cos -- IBM COS client created - Region: eu-de\n",
      "2022-01-07 15:27:11,890 [INFO] lithops.serverless.backends.ibm_cf.ibm_cf -- IBM CF client created - Region: us-south - Namespace: roy.rubin@post.idc.ac.il_dev\n",
      "2022-01-07 15:27:11,891 [INFO] lithops.invokers -- ExecutorID aa2599-1 | JobID A000 - Selected Runtime: lithopscloud/ibmcf-python-v37 - <function a at 0x0000026ADD4F3438>MB\n",
      "2022-01-07 15:27:12,689 [INFO] lithops.invokers -- Runtime lithopscloud/ibmcf-python-v37 with <function a at 0x0000026ADD4F3438>MB is not yet installed\n",
      "2022-01-07 15:27:12,690 [INFO] lithops.serverless.backends.ibm_cf.ibm_cf -- Creating new Lithops runtime based on Docker image lithopscloud/ibmcf-python-v37\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Object of type function is not JSON serializable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13128/3697607025.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmapreduce\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMapReduceEngine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mstatus\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmapreduce\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13128/200347441.py\u001b[0m in \u001b[0;36mexecute\u001b[1;34m(self, input_data, map_function, reduce_function)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m             \u001b[0mfexec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_at_parallel_map\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_function\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\lithops\\executors.py\u001b[0m in \u001b[0;36mcall_async\u001b[1;34m(self, func, data, extra_env, runtime_memory, timeout, include_modules, exclude_modules)\u001b[0m\n\u001b[0;32m    203\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlast_call\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'call_async'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 205\u001b[1;33m         \u001b[0mruntime_meta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minvoker\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect_runtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mruntime_memory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    206\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m         job = create_map_job(config=self.config,\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\lithops\\invokers.py\u001b[0m in \u001b[0;36mselect_runtime\u001b[1;34m(self, job_id, runtime_memory)\u001b[0m\n\u001b[0;32m    119\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m' with {}MB'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mruntime_memory\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mruntime_memory\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m' is not yet installed'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 121\u001b[1;33m             \u001b[0mruntime_meta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_runtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mruntime_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mruntime_memory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mruntime_timeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    122\u001b[0m             \u001b[0mruntime_meta\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'runtime_timeout'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mruntime_timeout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minternal_storage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mput_runtime_meta\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mruntime_key\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mruntime_meta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\lithops\\serverless\\serverless.py\u001b[0m in \u001b[0;36mcreate_runtime\u001b[1;34m(self, runtime_name, memory, timeout)\u001b[0m\n\u001b[0;32m     71\u001b[0m         \u001b[1;32mreturn\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mname\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mruntime\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m         \"\"\"\n\u001b[1;32m---> 73\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_runtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mruntime_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdelete_runtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mruntime_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\lithops\\serverless\\backends\\ibm_cf\\ibm_cf.py\u001b[0m in \u001b[0;36mcreate_runtime\u001b[1;34m(self, docker_image_name, memory, timeout)\u001b[0m\n\u001b[0;32m    156\u001b[0m             \u001b[0maction_bin\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maction_zip\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    157\u001b[0m         self.cf_client.create_action(self.package, action_name, docker_image_name, code=action_bin,\n\u001b[1;32m--> 158\u001b[1;33m                                      memory=memory, is_binary=True, timeout=timeout * 1000)\n\u001b[0m\u001b[0;32m    159\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_delete_function_handler_zip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\lithops\\libs\\openwhisk\\client.py\u001b[0m in \u001b[0;36mcreate_action\u001b[1;34m(self, package, action_name, image_name, code, memory, timeout, kind, is_binary, overwrite)\u001b[0m\n\u001b[0;32m     94\u001b[0m                         action_name + \"?overwrite=\" + str(overwrite)])\n\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 96\u001b[1;33m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     97\u001b[0m         \u001b[0mresp_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda_app\\envs\\Or\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36mput\u001b[1;34m(self, url, data, **kwargs)\u001b[0m\n\u001b[0;32m    600\u001b[0m         \"\"\"\n\u001b[0;32m    601\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 602\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'PUT'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    603\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    604\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda_app\\envs\\Or\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    526\u001b[0m             \u001b[0mhooks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhooks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    527\u001b[0m         )\n\u001b[1;32m--> 528\u001b[1;33m         \u001b[0mprep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprepare_request\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    529\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    530\u001b[0m         \u001b[0mproxies\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mproxies\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda_app\\envs\\Or\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36mprepare_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    464\u001b[0m             \u001b[0mauth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmerge_setting\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mauth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    465\u001b[0m             \u001b[0mcookies\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmerged_cookies\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 466\u001b[1;33m             \u001b[0mhooks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmerge_hooks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhooks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    467\u001b[0m         )\n\u001b[0;32m    468\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda_app\\envs\\Or\\lib\\site-packages\\requests\\models.py\u001b[0m in \u001b[0;36mprepare\u001b[1;34m(self, method, url, headers, files, data, params, auth, cookies, hooks, json)\u001b[0m\n\u001b[0;32m    317\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprepare_headers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    318\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprepare_cookies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcookies\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 319\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprepare_body\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    320\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprepare_auth\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mauth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda_app\\envs\\Or\\lib\\site-packages\\requests\\models.py\u001b[0m in \u001b[0;36mprepare_body\u001b[1;34m(self, data, files, json)\u001b[0m\n\u001b[0;32m    469\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    470\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 471\u001b[1;33m               \u001b[0mbody\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcomplexjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_nan\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    472\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mve\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    473\u001b[0m               \u001b[1;32mraise\u001b[0m \u001b[0mInvalidJSONError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mve\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda_app\\envs\\Or\\lib\\json\\__init__.py\u001b[0m in \u001b[0;36mdumps\u001b[1;34m(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[0;32m    236\u001b[0m         \u001b[0mcheck_circular\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcheck_circular\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_nan\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mallow_nan\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mindent\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    237\u001b[0m         \u001b[0mseparators\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mseparators\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdefault\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msort_keys\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msort_keys\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 238\u001b[1;33m         **kw).encode(obj)\n\u001b[0m\u001b[0;32m    239\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    240\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda_app\\envs\\Or\\lib\\json\\encoder.py\u001b[0m in \u001b[0;36mencode\u001b[1;34m(self, o)\u001b[0m\n\u001b[0;32m    197\u001b[0m         \u001b[1;31m# exceptions aren't as detailed.  The list call should be roughly\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m         \u001b[1;31m# equivalent to the PySequence_Fast that ''.join() would do.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m         \u001b[0mchunks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_one_shot\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m             \u001b[0mchunks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda_app\\envs\\Or\\lib\\json\\encoder.py\u001b[0m in \u001b[0;36miterencode\u001b[1;34m(self, o, _one_shot)\u001b[0m\n\u001b[0;32m    255\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkey_separator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem_separator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_keys\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    256\u001b[0m                 self.skipkeys, _one_shot)\n\u001b[1;32m--> 257\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_iterencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    259\u001b[0m def _make_iterencode(markers, _default, _encoder, _indent, _floatstr,\n",
      "\u001b[1;32m~\\anaconda_app\\envs\\Or\\lib\\json\\encoder.py\u001b[0m in \u001b[0;36mdefault\u001b[1;34m(self, o)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m         \"\"\"\n\u001b[1;32m--> 179\u001b[1;33m         raise TypeError(f'Object of type {o.__class__.__name__} '\n\u001b[0m\u001b[0;32m    180\u001b[0m                         f'is not JSON serializable')\n\u001b[0;32m    181\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Object of type function is not JSON serializable"
     ]
    }
   ],
   "source": [
    "mapreduce = MapReduceEngine()\n",
    "status = mapreduce.execute(input_data, a, a)\n",
    "print(status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['myCSV0.csv',\n",
       " 'myCSV1.csv',\n",
       " 'myCSV10.csv',\n",
       " 'myCSV11.csv',\n",
       " 'myCSV12.csv',\n",
       " 'myCSV13.csv',\n",
       " 'myCSV14.csv',\n",
       " 'myCSV15.csv',\n",
       " 'myCSV16.csv',\n",
       " 'myCSV17.csv',\n",
       " 'myCSV18.csv',\n",
       " 'myCSV19.csv',\n",
       " 'myCSV2.csv',\n",
       " 'myCSV3.csv',\n",
       " 'myCSV4.csv',\n",
       " 'myCSV5.csv',\n",
       " 'myCSV6.csv',\n",
       " 'myCSV7.csv',\n",
       " 'myCSV8.csv',\n",
       " 'myCSV9.csv']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure that `MapReduce Completed` should be printed and `mapreducefinal` folder should contain the result files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use python to delete all temporary data from mapreducetemp folder and delete SQLite database:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# remove temporary folders & files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete all csv files\n",
    "for i_file in input_data:\n",
    "    if os.path.exists(i_file):\n",
    "        os.remove(i_file)\n",
    "if os.path.exists(db_file_name):\n",
    "    os.remove(db_file_name)\n",
    "for i_folder in map_reduce_folder_names:\n",
    "    if os.path.exists(i_folder):\n",
    "        shutil.rmtree(i_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>\n",
    "\n",
    "# Question 5\n",
    "# Final Thoughts\n",
    "\n",
    "The phase where `MapReduceEngine` reads all temporary files generated by maps and sort them to provide each reducer a specific key is called the **shuffle step**.\n",
    "\n",
    "Please explain **clearly** what would be the main problem of MapReduce when processing Big Data, if there is no shuffle step at all, meaning reducers will directly read responses from the mappers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The main problem of MapReduce if there is no shuffle step is:\n",
    "\n",
    "> **the reduce step will not be performed as expected (we could say it failed)**.\n",
    "\n",
    "> See image below (from the lecture):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://1.bp.blogspot.com/-UvgLSDv7Rb4/Tbpn3veAOTI/AAAAAAAAAVk/kdaMzLa50BE/s1600/WordCountFlow.JPG\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "We can see in the example diagram above that if a reduce worker will recieve the output of the map worker directly, then the reduce function will return the wrong output.\n",
    "<br>\n",
    "for example:\n",
    "the bottom map worker will output:\n",
    "\n",
    " - apple,1\n",
    " \n",
    " - apple,1\n",
    " \n",
    " - plum,1\n",
    " \n",
    "the bottom reduce worker will recieve this as input, and might output:\n",
    "\n",
    " - apple,2\n",
    " \n",
    " - plum,1\n",
    " \n",
    "<br>\n",
    "After the reduce worker finished there is no other reduce step.\n",
    "<br>\n",
    "And so, If we will do the same for all map and reduce workers, we will get the following WRONG unreduced and unsummed output:\n",
    "\n",
    " - apple,1\n",
    " \n",
    " - orange,1\n",
    " \n",
    " - mango,1\n",
    " \n",
    " - orange,1\n",
    " \n",
    " - graspes,1\n",
    " \n",
    " - plum,1\n",
    " \n",
    " - apple,1\n",
    " \n",
    " - plum,1\n",
    " \n",
    " - mango,1\n",
    " \n",
    " - apple,2\n",
    " \n",
    " - plum,1\n",
    " \n",
    "<br>\n",
    "which is obviously not the wanted output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>\n",
    "Thank You :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
