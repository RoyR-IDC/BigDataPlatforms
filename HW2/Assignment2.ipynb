{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big Data Platform\n",
    "## Assignment 2: MapReduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**By:**  \n",
    "\n",
    "Or Livne, 300123123  \n",
    "Roy Rubin, 201312907\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The goal of this assignment is to:**\n",
    "- Understand and practice the details of MapReduceEngine\n",
    "\n",
    "**Instructions:**\n",
    "- Students will form teams of two people each, and submit a single homework for each team.\n",
    "- The same score for the homework will be given to each member of your team.\n",
    "- Your solution is in the form of a Jupyter notebook file (with extension ipynb).\n",
    "- Images/Graphs/Tables should be submitted inside the notebook.\n",
    "- The notebook should be runnable and properly documented. \n",
    "- Please answer all the questions and include all your code.\n",
    "- You are expected to submit a clear and pythonic code.\n",
    "- You can change functions signatures/definitions.\n",
    "\n",
    "**Submission:**\n",
    "- Submission of the homework will be done via Moodle by uploading a Jupyter notebook.\n",
    "- The homework needs to be entirely in English.\n",
    "- The deadline for submission is on Moodle.\n",
    "- Late submission won't be allowed.\n",
    "  \n",
    "  \n",
    "- In case of identical code submissions - both groups will get a Zero. \n",
    "- Some groups might be selected randomly to present their code.\n",
    "\n",
    "**Requirements:**  \n",
    "- Python 3.6 should be used.  \n",
    "- You should implement the algorithms by yourself using only basic Python libraries (such as numpy,pandas,etc.)\n",
    "\n",
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Grading:**\n",
    "- Q1 - 5 points - Initial Steps\n",
    "- Q2 - 50 points - MapReduceEngine\n",
    "- Q3 - 30 points - Implement the MapReduce Inverted index of the JSON documents\n",
    "- Q4 - 5 points - Testing Your MapReduce\n",
    "- Q5 - 10 points - Final Thoughts \n",
    "\n",
    "`Total: 100`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prerequisites**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # example\n",
    "!pip install --quiet zipfile36"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import warnings\n",
    "import threading # you can use easier threading packages\n",
    "\n",
    "# ml\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "\n",
    "# visual\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# notebook\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hide Warnings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Disable Autoscrolling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
       "    return false;\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Set Random Seed**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import random\n",
    "import os \n",
    "import gc\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "import dask\n",
    "from dask.dataframe import to_parquet\n",
    "import csv\n",
    "from dask.dataframe import from_pandas\n",
    "import gc \n",
    "import glob\n",
    "from shutil import copyfile\n",
    "import shutil\n",
    "gc.collect()\n",
    "\n",
    "# general\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import warnings\n",
    "import threading # you can use easier threading packages\n",
    "import string \n",
    "\n",
    "# ml\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "\n",
    "# visual\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# notebook\n",
    "from IPython.display import display\n",
    "from pathlib import Path\n",
    "warnings.filterwarnings('ignore')\n",
    "from joblib import Parallel, delayed\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# assitence function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class single_record():\n",
    "    firstname = ''\n",
    "    secoundname = ''\n",
    "    city = ''\n",
    "    \n",
    "    # Set the value options\n",
    "    firstname_option_list  = ['John', 'Dana', 'Scott', 'Marc', 'Steven', 'Michael', 'Albert', 'Johanna']\n",
    "    secoundname_option_list  = ['John', 'Dana', 'Scott', 'Marc', 'Steven', 'Michael', 'Albert', 'Johanna']\n",
    "    city_option_list = ['NewYork', 'Haifa', 'Munchen', 'London', 'PaloAlto',  'TelAviv', 'Kiel', 'Hamburg']\n",
    "    def __init__(self, id):\n",
    "        self.firstname = random.choice(self.firstname_option_list)\t\n",
    "        random_name  = ''.join(random.choice(string.ascii_uppercase + string.ascii_lowercase) for _ in range(random.randint(2,4)))\n",
    "        self.secoundname = random_name\n",
    "        self.city = random.choice(self.city_option_list)\n",
    "        \n",
    "def init_data_set_configuration():\n",
    "    # Set the value options\n",
    "    global max_rows, csv_columns, db_file_name, csv_file_name, columns_type\n",
    "    global parquet_file_name_using_dask,parquet_file_name_using_pyarray \n",
    "    global parquet_file_name_using_pandas, csv_index, csv_ending, amount_of_files\n",
    "    global map_reduce_folder_names, amount_of_process, map_regex, db_columns\n",
    "    global db_columns_type, db_table_name, reduce_regex_init, reduce_regex_final\n",
    "    \n",
    "    \n",
    "    Current_python_file_path = os.getcwd()\n",
    "    max_rows = 10\n",
    "    amount_of_files = 5 # neeed to be 20\n",
    "    amount_of_process = 2\n",
    "    csv_columns = ['firstname','secondname','city']\n",
    "    db_columns = ['key', 'value']\n",
    "    map_reduce_folder_names =  [Current_python_file_path+ '\\\\mapreducetemp', Current_python_file_path+'\\\\mapreducefinal']\n",
    "\n",
    "    db_columns_type = [ 'text',  'text']\n",
    "\n",
    "    db_file_name = 'mydata.db'\n",
    "    csv_file_name = 'myCSV'\n",
    "    csv_ending = '.csv'\n",
    "    map_regex = 'part-tmp-'\n",
    "    reduce_regex_init = 'part-'\n",
    "    reduce_regex_final = '-final'\n",
    "\n",
    "    db_table_name = 'temp_results'\n",
    "\n",
    "    #parquet_file_name_using_dask = 'mydatapyarrow_dask.parquet'\n",
    "    #parquet_file_name_using_pyarray = 'mydatapyarrow_pyarray.parquet'\n",
    "    #parquet_file_name_using_pandas = 'mydatapyarrow_pandas.parquet'\n",
    "\n",
    "    return\n",
    "\n",
    "def create_csvdatabase_file(max_rows):\n",
    "    # this loop generate single fruit\n",
    "    create_n_list_in_advance = max_rows*[None]\n",
    "    for i_row_index in range(max_rows):\n",
    "        i_record = single_record(i_row_index)\n",
    "        create_n_list_in_advance[i_row_index] = [i_record.firstname, i_record.secoundname, \n",
    "                                                 i_record.city]\n",
    "    \n",
    "    mydata_df = pd.DataFrame(create_n_list_in_advance, columns = csv_columns )\n",
    "    mydata_df.to_csv(csv_file_name)\n",
    "    return mydata_df\n",
    "\n",
    "def generate_n_csv_file():\n",
    "    for i_csv_index in range(0, amount_of_files):\n",
    "        mydata_df = create_csvdatabase_file(max_rows)\n",
    "        mydata_df.to_csv(csv_file_name + str(i_csv_index) + csv_ending )\n",
    "    return\n",
    "\n",
    "def create_new_folder(path):\n",
    "    Path(path).mkdir(parents=True, exist_ok=True)\n",
    "    return \n",
    "\n",
    "def generate_map_reduce_folders():\n",
    "    for i_folder in map_reduce_folder_names:\n",
    "        create_new_folder(i_folder)\n",
    "    return \n",
    "\n",
    "def create_db_database(db_file_name):\n",
    "    is_exist = os.path.exists(db_file_name)\n",
    "    if  is_exist:\n",
    "        os.remove(db_file_name)\n",
    "    con = sqlite3.connect(db_file_name)\n",
    "        \n",
    "    \n",
    "    cur = con.cursor()\n",
    "    \n",
    "    # Create table\n",
    "    columns_type_list = list(map(lambda x,y: x+' ' + y, db_columns, db_columns_type))\n",
    "    columns_type_list_string = \"(\"+\", \".join(map(str, columns_type_list))+\")\"\n",
    "\n",
    "    cur.execute(''' \n",
    "                CREATE TABLE temp_results\n",
    "                ''' + columns_type_list_string + \\\n",
    "               '''''')\n",
    "    \n",
    "    con.commit()\n",
    "    con.close()\n",
    "    return con, cur\n",
    "\n",
    "\n",
    "def fill_db_data_base_using_csv_data_base_with_same_keys(mydata_df):\n",
    "    con = sqlite3.connect(db_file_name)\n",
    "    cur = con.cursor()\n",
    "\n",
    "    for i_csv_row in range(mydata_df.shape[0]):\n",
    "        # Take row from csv file\n",
    "        i_row = mydata_df.iloc[i_csv_row]\n",
    "        i_row_as_list = i_row.to_list()\n",
    "        \n",
    "        i_row_as_list_string = \"('\"+\"','\".join(map(str, i_row_as_list))+\"')\"\n",
    "        #cur.execute(\"INSERT INTO stocks VALUES ('2006-01-05','BUY','RHAT','100')\")\n",
    "\n",
    "        #Insert a csv row of data base\n",
    "        cur.execute(\"INSERT INTO temp_results VALUES \"  \\\n",
    "                    +i_row_as_list_string + \\\n",
    "                    \"\")\n",
    "    con.commit()\n",
    "    con.close()\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def map_function(filename: str) -> dict:\n",
    "    #print(filename)\n",
    "    df = pd.read_csv(filename)\n",
    "    Dict = {'key': df['firstname'].to_list(), 'value': [filename]*len(df)}\n",
    "    return Dict\n",
    "\n",
    "\n",
    "def reduce_function(key: str, value: str) -> dict:\n",
    "    #print(filename)\n",
    "    amount_of_csv_files = value.split(',').__len__()\n",
    "    Dict = {'key': key, 'value': amount_of_csv_files}\n",
    "    return Dict\n",
    "\n",
    "def run_at_parallel_map(i_item_index, i_item, function):\n",
    "    new_path = None\n",
    "    succeed = True\n",
    "    try:\n",
    "        \n",
    "        new_path  = map_reduce_folder_names[0] + '\\\\'+ map_regex + str(i_item_index) + csv_ending\n",
    "    \n",
    "        dict_result = function(i_item)\n",
    "        result_df = pd.DataFrame(data=dict_result, columns = ['key', 'value'])\n",
    "\n",
    "        result_df.to_csv(new_path) \n",
    "        \n",
    "        succeed = os.path.exists(new_path)\n",
    "    except:\n",
    "        succeed = False\n",
    "        \n",
    "    return  succeed, new_path\n",
    "\n",
    "\n",
    "def run_at_parallel_reduce(i_item_index, i_item, function):\n",
    "    new_path = None\n",
    "    succeed = True\n",
    "    try:\n",
    "        new_path  = map_reduce_folder_names[1] + '\\\\'+ reduce_regex_init + str(i_item_index) + reduce_regex_final + csv_ending\n",
    "        #print(new_path)\n",
    "        key = i_item[0]\n",
    "        value = i_item[1]\n",
    "\n",
    "        dict_result = function(key, value)\n",
    "        result_df = pd.DataFrame.from_records([dict_result])\n",
    "\n",
    "        result_df.to_csv(new_path) \n",
    "        \n",
    "        succeed = os.path.exists(new_path)\n",
    "    except:\n",
    "        succeed = False\n",
    "        \n",
    "    return  succeed, new_path\n",
    "\n",
    "\n",
    "def print_db_file_info_bsase_single_key(key):\n",
    "    con = sqlite3.connect(db_file_name)\n",
    "    cur = con.cursor()\n",
    "    return_list = []\n",
    "    for row in cur.execute('SELECT key, GROUP_CONCAT(value) FROM ' +db_table_name+ ' GROUP BY ' + key + ' ORDER BY ' + key):\n",
    "        #print(row)\n",
    "        return_list.append(row)\n",
    "    con.close()\n",
    "    return return_list\n",
    "\n",
    "def print_db_file_info():\n",
    "    con = sqlite3.connect(db_file_name)\n",
    "    cur = con.cursor()\n",
    "    for row in cur.execute('SELECT * FROM '+db_table_name):\n",
    "        print(row)\n",
    "    con.close()\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>\n",
    "# Question 1\n",
    "# Initial Steps\n",
    "\n",
    "Write Python code to create 20 different CSV files in this format:  `myCSV[Number].csv`, where each file contains 10 records. \n",
    "\n",
    "The schema is `(‘firstname’,’secondname’,city’)`  \n",
    "\n",
    "Values should be randomly chosen from the lists: \n",
    "- `firstname` : `[John, Dana, Scott, Marc, Steven, Michael, Albert, Johanna]`  \n",
    "- `city` : `[New York, Haifa, München, London, Palo Alto,  Tel Aviv, Kiel, Hamburg]`  \n",
    "- `secondname`: any value  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# init_data_set_configuration settings\n",
    "init_data_set_configuration()\n",
    "\n",
    "# \n",
    "generate_n_csv_file()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use python to Create `mapreducetemp` and `mapreducefinal` folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_map_reduce_folders()\n",
    "\n",
    "\n",
    "# create csv file as data frame \n",
    "\n",
    "input_data = glob.glob(csv_file_name+'*'+csv_ending  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "# Question 2\n",
    "## MapReduceEngine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Write Python code to create an SQLite database with the following table\n",
    "\n",
    "`TableName: temp_results`   \n",
    "`schema: (key:TEXT,value:TEXT)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_conn, cur = create_db_database(db_file_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Create a Python class** `MapReduceEngine` with method `def execute(input_data, map_function, reduce_function)`, such that:\n",
    "    - `input_data`: is an array of elements\n",
    "    - `map_function`: is a pointer to the Python function that returns a list where each entry of the form (key,value) \n",
    "    - `reduce_function`: is pointer to the Python function that returns a list where each entry of the form (key,value)\n",
    "\n",
    "<br><br>\n",
    "\n",
    "3.**Implement** the following functionality in the `execute(...)` function:\n",
    "\n",
    "<br>\n",
    "\n",
    "1. For each key  from the  input_data, start a new Python thread that executes map_function(key) \n",
    "<br><br>\n",
    "2. Each thread will store results of the map_function into mapreducetemp/part-tmp-X.csv where X is a unique number per each thread.\n",
    "<br><br>\n",
    "3. Keep the list of all threads and check whether they are completed.\n",
    "<br><br>\n",
    "4. Once all threads completed, load content of all CSV files into the temp_results table in SQLite.\n",
    "\n",
    "    Remark: Easiest way to loop over all CSV files and load them into Pandas first, then load into SQLite  \n",
    "    `data = pd.read_csv(path to csv)`  \n",
    "    `data.to_sql(‘temp_results’,sql_conn, if_exists=’append’,index=False)`\n",
    "<br><br>\n",
    "\n",
    "5. **Write SQL statement** that generates a sorted list by key of the form `(key, value)` where value is concatenation of ALL values in the value column that match specific key. For example, if table has records\n",
    "<table>\n",
    "    <tbody>\n",
    "            <tr>\n",
    "                <td style=\"text-align:center\">John</td>\n",
    "                <td style=\"text-align:center\">myCSV1.csv</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td style=\"text-align:center\">Dana</td>\n",
    "                <td style=\"text-align:center\">myCSV5.csv</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td style=\"text-align:center\">John</td>\n",
    "                <td style=\"text-align:center\">myCSV7.csv</td>\n",
    "            </tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "\n",
    "    Then SQL statement will return `(‘John’,’myCSV1.csv, myCSV7.csv’)`  \n",
    "    Remark: use GROUP_CONCAT and also GROUP BY ORDER BY\n",
    "<br><br><br>\n",
    "6. **Start a new thread** for each value from the generated list in the previous step, to execute `reduce_function(key,value)` \n",
    "<br>    \n",
    "7. Each thread will store results of reduce_function into `mapreducefinal/part-X-final.csv` file  \n",
    "<br>\n",
    "8. Keep list of all threads and check whether they are completed  \n",
    "<br>\n",
    "9. Once all threads completed, print on the screen `MapReduce Completed` otherwise print `MapReduce Failed` \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement all of the class here\n",
    "\n",
    "class MapReduceEngine():\n",
    "    def execute(self, input_data, map_function, reduce_function):\n",
    "        \n",
    "        \n",
    "        #  1) For each key  from the  input_data, start a new Python thread that executes \n",
    "        #     map_function(key)\n",
    "        #  2) Each thread will store results of the map_function into \n",
    "        #     mapreducetemp/part-tmp-X.csv where X\n",
    "        #     is a unique number per each thread.\n",
    "        #  3) Keep the list of all threads and check whether they are completed\n",
    "        \n",
    "        succeed_new_path_list  = Parallel(n_jobs=amount_of_process, backend=\"threading\", \\\n",
    "                                          prefer=\"processes\")(delayed(run_at_parallel_map)(\n",
    "            index, item, map_function) for index, item in enumerate(input_data))\n",
    "        \n",
    "        \n",
    "        # 4) Once all threads completed, load content of all CSV files into the temp_results\n",
    "        #    table in SQLite\n",
    "        \n",
    "        # get new files names\n",
    "        filepaths = [path for boolean, path in succeed_new_path_list]\n",
    "        \n",
    "        # write generated csv files to sql data base\n",
    "        sql_conn = sqlite3.connect(db_file_name)\n",
    "        list(map(lambda x: pd.read_csv(x, index_col=0) \\\n",
    "                 .to_sql('temp_results',sql_conn, if_exists='append',index=False), filepaths ))    \n",
    "        sql_conn.close()\n",
    "        \n",
    "        # get list of succeed or failed of threads\n",
    "        boolean_results = [boolean for boolean, path in succeed_new_path_list]\n",
    "        \n",
    "        # validate that all threads are completed succesfully\n",
    "        if False in boolean_results:\n",
    "            status = 'Map Reduce Failed'\n",
    "            return status\n",
    "\n",
    "        #print_db_file_info()\n",
    "        \n",
    "        # 5) **Write SQL statement** that generates a sorted list by key of the form \n",
    "        #    `(key, value)` where value is concatenation of ALL values in the value column\n",
    "        #     that match specific key. For example, if table has records\n",
    "        \n",
    "        # query data base using GROUP_CONCAT and GROUP BY  and ORDER BY \n",
    "        generates_list = print_db_file_info_bsase_single_key(key='key')\n",
    "        \n",
    "        # 6) **Start a new thread** for each value from\n",
    "        #    the generated list in the previous step, to execute `reduce_function(key,value)\n",
    "        #    Begin by Performing REDUCE actions\n",
    "        #    we will open a thread for each REDUCE\n",
    "        # 7) Each thread will store results of reduce_function into \n",
    "        #   `mapreducefinal/part-X-final.csv` file\n",
    "        \n",
    "        # 8) Keep list of all threads and check whether they are completed\n",
    "        \n",
    "        reduce_return_dict_succeed  = Parallel(n_jobs=amount_of_process, backend=\"threading\",\\\n",
    "                                               prefer=\"processes\")(delayed(run_at_parallel_reduce)(\n",
    "            index, item, reduce_function) for index, item in enumerate(generates_list))\n",
    "\n",
    "        \n",
    "        # 9) Once all threads completed, print on the screen \n",
    "        #   `MapReduce Completed` otherwise print `MapReduce Failed`\n",
    "        boolean_results = [boolean for boolean, path in reduce_return_dict_succeed]\n",
    "        \n",
    "        if False in boolean_results:\n",
    "            status = 'Map Reduce Failed'\n",
    "            return status\n",
    "        \n",
    "        reduce_dict = [reduce_dict for boolean, reduce_dict in reduce_return_dict_succeed]\n",
    "    \n",
    "        reduce_df = pd.DataFrame(data= reduce_dict)\n",
    "        status = 'Map Reduce Completed'\n",
    "        \n",
    "        return status\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>\n",
    "\n",
    "# Question 3\n",
    "## Implement the MapReduce Inverted index of the JSON documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a function `inverted_map(document_name)` which reads the CSV document from the local disc and return a list that contains entries of the form (key_value, document name).\n",
    "\n",
    "For example, if myCSV4.csv document has values like:  \n",
    "`{‘firstname’:’John’,‘secondname’:’Rambo’,‘city’:’Palo Alto’}`\n",
    "\n",
    "Then `inverted_map(‘myCSV4.csv’)` function will return a list:  \n",
    "`[(‘firstname_John’,’ myCSV4.csv’),(‘secondname_Rambo’,’ myCSV4.csv’), (‘city_Palo Alto’,’ myCSV4.csv’)]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverted_map(document_name):\n",
    "    #ducument_name = input_data[0]\n",
    "    csv_df = pd.read_csv(document_name, index_col=0)\n",
    "    csv_size = csv_df.shape[0]\n",
    "    csv_columns = csv_df.columns.to_list()\n",
    "    output_list = []\n",
    "    for i_col in csv_columns:\n",
    "        col_vals  = csv_df[i_col].to_list()\n",
    "        curr_ouput = list(map(lambda x,y,z: (x+ '_' + y, z) , \\\n",
    "                              csv_size*[i_col], col_vals, csv_size*[document_name]))\n",
    "        output_list += curr_ouput\n",
    "    \n",
    "    return output_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a reduce function `inverted_reduce(value, documents)`, where the field “documents” contains a list of all CSV documents per given value.   \n",
    "This list might have duplicates.   \n",
    "Reduce function will return new list without duplicates.\n",
    "\n",
    "For example,  \n",
    "calling the function `inverted_reduce(‘firstname_Albert’,’myCSV2.csv, myCSV5.csv,myCSV2.csv’)`   \n",
    "will return a list `[‘firstname_Albert’,’myCSV2.csv, myCSV5.csv,myCSV2.csv’]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverted_reduce(value, documents):\n",
    "    ducument_name_list = documents.split(',')\n",
    "    ducument_name_list_no_duplicates = list(set(ducument_name_list))\n",
    "    string_ducument_name_list_no_duplicates = (', ').join(ducument_name_list_no_duplicates)\n",
    "    return_list = [value, string_ducument_name_list_no_duplicates]\n",
    "    return return_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>\n",
    "# Question 4\n",
    "## Testing Your MapReduce\n",
    "\n",
    "**Create Python list** `input_data` : `[‘myCSV1.csv’,.. ,‘myCSV20.csv’]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Submit MapReduce as follows:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Map Reduce Completed\n"
     ]
    }
   ],
   "source": [
    "mapreduce = MapReduceEngine()\n",
    "status = mapreduce.execute(input_data, inverted_map, inverted_reduce)\n",
    "print(status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure that `MapReduce Completed` should be printed and `mapreducefinal` folder should contain the result files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use python to delete all temporary data from mapreducetemp folder and delete SQLite database:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# remove temporary folders & files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# dellete all csv files\n",
    "for i_file in input_data:\n",
    "    if os.path.exists(i_file):\n",
    "        os.remove(i_file)\n",
    "if os.path.exists(db_file_name):\n",
    "    os.remove(db_file_name)\n",
    "for i_folder in map_reduce_folder_names:\n",
    "    if os.path.exists(i_folder):\n",
    "        shutil.rmtree(i_folder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>\n",
    "\n",
    "# Question 5\n",
    "# Final Thoughts\n",
    "\n",
    "The phase where `MapReduceEngine` reads all temporary files generated by maps and sort them to provide each reducer a specific key is called the **shuffle step**.\n",
    "\n",
    "Please explain **clearly** what would be the main problem of MapReduce when processing Big Data, if there is no shuffle step at all, meaning reducers will directly read responses from the mappers.\n",
    "\n",
    "** note by roy: see image from lecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main problem of MapReduce if there is no shuffle step is that the reduce step will not be performed as expected (we could say it failed).\n",
    "\n",
    "See image below (from the lecture):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://1.bp.blogspot.com/-UvgLSDv7Rb4/Tbpn3veAOTI/AAAAAAAAAVk/kdaMzLa50BE/s1600/WordCountFlow.JPG\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see in the example diagram above that if a reduce worker will recieve the output of the map worker directly, then the reduce function will return the wrong output.\n",
    "for example:\n",
    "the bottom map worker will output:\n",
    "apple,1\n",
    "apple,1\n",
    "plum,1\n",
    "the bottom reduce worker will recieve this as input, and might output:\n",
    "apple,2\n",
    "plum,1\n",
    "After the reduce worker finished there is no other reduce step.\n",
    "If we will do the same for all map and reduce workers, we will get the following WRONG unreduced and unsummed output:\n",
    "apple,1\n",
    "orange,1\n",
    "mango,1\n",
    "orange,1\n",
    "graspes,1\n",
    "plum,1\n",
    "apple,1\n",
    "plum,1\n",
    "mango,1\n",
    "## apple,2\n",
    "plum,1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>\n",
    "Good Luck :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
