{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big Data Platform\n",
    "## Assignment 2: MapReduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**By:**  \n",
    "\n",
    "Or Livne, 300123123  \n",
    "Roy Rubin, 201312907\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The goal of this assignment is to:**\n",
    "- Understand and practice the details of MapReduceEngine\n",
    "\n",
    "**Instructions:**\n",
    "- Students will form teams of two people each, and submit a single homework for each team.\n",
    "- The same score for the homework will be given to each member of your team.\n",
    "- Your solution is in the form of a Jupyter notebook file (with extension ipynb).\n",
    "- Images/Graphs/Tables should be submitted inside the notebook.\n",
    "- The notebook should be runnable and properly documented. \n",
    "- Please answer all the questions and include all your code.\n",
    "- You are expected to submit a clear and pythonic code.\n",
    "- You can change functions signatures/definitions.\n",
    "\n",
    "**Submission:**\n",
    "- Submission of the homework will be done via Moodle by uploading a Jupyter notebook.\n",
    "- The homework needs to be entirely in English.\n",
    "- The deadline for submission is on Moodle.\n",
    "- Late submission won't be allowed.\n",
    "  \n",
    "  \n",
    "- In case of identical code submissions - both groups will get a Zero. \n",
    "- Some groups might be selected randomly to present their code.\n",
    "\n",
    "**Requirements:**  \n",
    "- Python 3.6 should be used.  \n",
    "- You should implement the algorithms by yourself using only basic Python libraries (such as numpy,pandas,etc.)\n",
    "\n",
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Grading:**\n",
    "- Q1 - 5 points - Initial Steps\n",
    "- Q2 - 50 points - MapReduceEngine\n",
    "- Q3 - 30 points - Implement the MapReduce Inverted index of the JSON documents\n",
    "- Q4 - 5 points - Testing Your MapReduce\n",
    "- Q5 - 10 points - Final Thoughts \n",
    "\n",
    "`Total: 100`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prerequisites**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Install joblib\n",
      "\n",
      "Requirement already satisfied: joblib in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (1.1.0)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(f'Install joblib\\n')\n",
    "!{sys.executable} -m pip install joblib\n",
    "#!pip install joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SQL\n",
    "import sqlite3\n",
    "\n",
    "# ML\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "\n",
    "# general\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import warnings\n",
    "import string \n",
    "import glob\n",
    "from shutil import copyfile\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from functools import partial\n",
    "\n",
    "# visual\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# notebook\n",
    "from IPython.display import display\n",
    "\n",
    "# threading\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hide Warnings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Disable Autoscrolling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
       "    return false;\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Set Random Seed**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Assisting Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_configurations():\n",
    "    # Set the value options\n",
    "    global max_rows, csv_columns, db_file_name, csv_file_name, columns_type\n",
    "    global csv_index, csv_ending, amount_of_files\n",
    "    global map_reduce_folder_names, amount_of_process, map_regex, db_columns\n",
    "    global db_columns_type, db_table_name, reduce_regex_init, reduce_regex_final\n",
    "    \n",
    "    \n",
    "    Current_python_file_path = os.getcwd()\n",
    "    max_rows = 10\n",
    "    amount_of_files = 20\n",
    "    amount_of_process = 2\n",
    "    csv_columns = ['firstname','secondname','city']\n",
    "    db_columns = ['key', 'value']\n",
    "    map_reduce_folder_names =  [os.path.join(Current_python_file_path, 'mapreducetemp'), \n",
    "                                os.path.join(Current_python_file_path, 'mapreducefinal')]\n",
    "\n",
    "    db_columns_type = [ 'text',  'text']\n",
    "\n",
    "    db_file_name = 'mydata.db'\n",
    "    csv_file_name = 'myCSV'\n",
    "    csv_ending = '.csv'\n",
    "    map_regex = 'part-tmp-'\n",
    "    reduce_regex_init = 'part-'\n",
    "    reduce_regex_final = '-final'\n",
    "\n",
    "    db_table_name = 'temp_results'\n",
    "\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set relevant configurations\n",
    "init_configurations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>\n",
    "# Question 1\n",
    "# Initial Steps\n",
    "\n",
    "Write Python code to create 20 different CSV files in this format:  `myCSV[Number].csv`, where each file contains 10 records. \n",
    "\n",
    "The schema is `(‘firstname’,’secondname’,city’)`  \n",
    "\n",
    "Values should be randomly chosen from the lists: \n",
    "- `firstname` : `[John, Dana, Scott, Marc, Steven, Michael, Albert, Johanna]`  \n",
    "- `city` : `[New York, Haifa, München, London, Palo Alto,  Tel Aviv, Kiel, Hamburg]`  \n",
    "- `secondname`: any value  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class single_record():\n",
    "    firstname = ''\n",
    "    secoundname = ''\n",
    "    city = ''\n",
    "    \n",
    "    # Set the value options\n",
    "    firstname_option_list  = ['John', 'Dana', 'Scott', 'Marc', 'Steven', 'Michael', 'Albert', 'Johanna']\n",
    "    secoundname_option_list  = ['John', 'Dana', 'Scott', 'Marc', 'Steven', 'Michael', 'Albert', 'Johanna']\n",
    "    city_option_list = ['NewYork', 'Haifa', 'Munchen', 'London', 'PaloAlto',  'TelAviv', 'Kiel', 'Hamburg']\n",
    "    def __init__(self, id):\n",
    "        self.firstname = random.choice(self.firstname_option_list)\t\n",
    "        random_name  = ''.join(random.choice(string.ascii_uppercase + string.ascii_lowercase) for _ in range(random.randint(2,4)))\n",
    "        self.secoundname = random_name\n",
    "        self.city = random.choice(self.city_option_list)\n",
    "\n",
    "def create_csvdatabase_file(max_rows):\n",
    "    # this loop generate single fruit\n",
    "    create_n_list_in_advance = max_rows*[None]\n",
    "    for i_row_index in range(max_rows):\n",
    "        i_record = single_record(i_row_index)\n",
    "        create_n_list_in_advance[i_row_index] = [i_record.firstname, i_record.secoundname, i_record.city]\n",
    "    \n",
    "    mydata_df = pd.DataFrame(create_n_list_in_advance, columns = csv_columns)\n",
    "    #mydata_df.to_csv(csv_file_name)  #TODO verify ROY commented this\n",
    "    return mydata_df\n",
    "\n",
    "def generate_n_csv_files():\n",
    "    for i_csv_index in range(0, amount_of_files):\n",
    "        mydata_df = create_csvdatabase_file(max_rows)\n",
    "        mydata_df.to_csv(csv_file_name + str(i_csv_index) + csv_ending)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate all csv files\n",
    "generate_n_csv_files()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use python to Create `mapreducetemp` and `mapreducefinal` folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_new_folder(path):\n",
    "    Path(path).mkdir(parents=True, exist_ok=True)\n",
    "    return \n",
    "\n",
    "def generate_map_reduce_folders():\n",
    "    for i_folder in map_reduce_folder_names:\n",
    "        print(f'Creating folder: {i_folder}')\n",
    "        create_new_folder(i_folder)\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating folder: /Users/royrubin/PycharmProjects/BigDataPlatforms/HW2/mapreducetemp\n",
      "Creating folder: /Users/royrubin/PycharmProjects/BigDataPlatforms/HW2/mapreducefinal\n"
     ]
    }
   ],
   "source": [
    "# generate folders\n",
    "generate_map_reduce_folders()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "# Question 2\n",
    "## MapReduceEngine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Write Python code to create an SQLite database with the following table\n",
    "\n",
    "`TableName: temp_results`   \n",
    "`schema: (key:TEXT,value:TEXT)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_database(db_file_name):\n",
    "    is_exist = os.path.exists(db_file_name)\n",
    "    if  is_exist:\n",
    "        os.remove(db_file_name)\n",
    "    con = sqlite3.connect(db_file_name)\n",
    "        \n",
    "    \n",
    "    cur = con.cursor()\n",
    "    \n",
    "    # Create table\n",
    "    columns_type_list = list(map(lambda x,y: x+' ' + y, db_columns, db_columns_type))\n",
    "    columns_type_list_string = \"(\"+\", \".join(map(str, columns_type_list))+\")\"\n",
    "\n",
    "    cur.execute(''' \n",
    "                CREATE TABLE temp_results\n",
    "                ''' + columns_type_list_string + \\\n",
    "               '''''')\n",
    "    \n",
    "    con.commit()\n",
    "    con.close()\n",
    "    return con, cur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_conn, cur = create_database(db_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Create a Python class** `MapReduceEngine` with method `def execute(input_data, map_function, reduce_function)`, such that:\n",
    "    - `input_data`: is an array of elements\n",
    "    - `map_function`: is a pointer to the Python function that returns a list where each entry of the form (key,value) \n",
    "    - `reduce_function`: is pointer to the Python function that returns a list where each entry of the form (key,value)\n",
    "\n",
    "<br><br>\n",
    "\n",
    "3.**Implement** the following functionality in the `execute(...)` function:\n",
    "\n",
    "<br>\n",
    "\n",
    "1. For each key  from the  input_data, start a new Python thread that executes map_function(key) \n",
    "<br><br>\n",
    "2. Each thread will store results of the map_function into mapreducetemp/part-tmp-X.csv where X is a unique number per each thread.\n",
    "<br><br>\n",
    "3. Keep the list of all threads and check whether they are completed.\n",
    "<br><br>\n",
    "4. Once all threads completed, load content of all CSV files into the temp_results table in SQLite.\n",
    "\n",
    "    Remark: Easiest way to loop over all CSV files and load them into Pandas first, then load into SQLite  \n",
    "    `data = pd.read_csv(path to csv)`  \n",
    "    `data.to_sql(‘temp_results’,sql_conn, if_exists=’append’,index=False)`\n",
    "<br><br>\n",
    "\n",
    "5. **Write SQL statement** that generates a sorted list by key of the form `(key, value)` where value is concatenation of ALL values in the value column that match specific key. For example, if table has records\n",
    "<table>\n",
    "    <tbody>\n",
    "            <tr>\n",
    "                <td style=\"text-align:center\">John</td>\n",
    "                <td style=\"text-align:center\">myCSV1.csv</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td style=\"text-align:center\">Dana</td>\n",
    "                <td style=\"text-align:center\">myCSV5.csv</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td style=\"text-align:center\">John</td>\n",
    "                <td style=\"text-align:center\">myCSV7.csv</td>\n",
    "            </tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "\n",
    "    Then SQL statement will return `(‘John’,’myCSV1.csv, myCSV7.csv’)`  \n",
    "    Remark: use GROUP_CONCAT and also GROUP BY ORDER BY\n",
    "<br><br><br>\n",
    "6. **Start a new thread** for each value from the generated list in the previous step, to execute `reduce_function(key,value)` \n",
    "<br>    \n",
    "7. Each thread will store results of reduce_function into `mapreducefinal/part-X-final.csv` file  \n",
    "<br>\n",
    "8. Keep list of all threads and check whether they are completed  \n",
    "<br>\n",
    "9. Once all threads completed, print on the screen `MapReduce Completed` otherwise print `MapReduce Failed` \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement all of the class here\n",
    "\n",
    "class MapReduceEngine():\n",
    "    def execute(self, input_data, map_function, reduce_function):\n",
    "        #  1) For each key  from the  input_data, start a new Python thread that executes \n",
    "        #     map_function(key)\n",
    "        #  2) Each thread will store results of the map_function into \n",
    "        #     mapreducetemp/part-tmp-X.csv where X\n",
    "        #     is a unique number per each thread.\n",
    "        #  3) Keep the list of all threads and check whether they are completed\n",
    "        \n",
    "        succeed_new_path_list  = Parallel(n_jobs=amount_of_process, backend=\"threading\", \\\n",
    "                                          prefer=\"processes\")(delayed(self._run_at_parallel_map)(\n",
    "            index, item, map_function) for index, item in enumerate(input_data))\n",
    "        \n",
    "        \n",
    "        # 4) Once all threads completed, load content of all CSV files into the temp_results\n",
    "        #    table in SQLite\n",
    "        \n",
    "        # get new files names\n",
    "        filepaths = [path for boolean, path in succeed_new_path_list]\n",
    "        \n",
    "        # write generated csv files to sql data base\n",
    "        sql_conn = sqlite3.connect(db_file_name)\n",
    "        list(map(lambda x: pd.read_csv(x, index_col=0) \\\n",
    "                 .to_sql('temp_results',sql_conn, if_exists='append',index=False), filepaths))    \n",
    "        sql_conn.close()\n",
    "        \n",
    "        # get list of succeed or failed of threads\n",
    "        boolean_results = [boolean for boolean, path in succeed_new_path_list]\n",
    "        \n",
    "        # validate that all threads are completed succesfully\n",
    "        if False in boolean_results:\n",
    "            status = 'Map Reduce Failed'\n",
    "            return status\n",
    "\n",
    "        #print_db_file_info()\n",
    "        \n",
    "        # 5) **Write SQL statement** that generates a sorted list by key of the form \n",
    "        #    `(key, value)` where value is concatenation of ALL values in the value column\n",
    "        #     that match specific key. For example, if table has records\n",
    "        \n",
    "        # query data base using GROUP_CONCAT and GROUP BY  and ORDER BY \n",
    "        generated_list = self._get_grouped_info_from_db_by_key(key='key')\n",
    "        \n",
    "        # 6) **Start a new thread** for each value from\n",
    "        #    the generated list in the previous step, to execute `reduce_function(key,value)\n",
    "        #    Begin by Performing REDUCE actions\n",
    "        #    we will open a thread for each REDUCE\n",
    "        # 7) Each thread will store results of reduce_function into \n",
    "        #   `mapreducefinal/part-X-final.csv` file\n",
    "        \n",
    "        # 8) Keep list of all threads and check whether they are completed\n",
    "        \n",
    "        reduce_return_dict_succeed  = Parallel(n_jobs=amount_of_process, backend=\"threading\",\\\n",
    "                                               prefer=\"processes\")(delayed(self._run_at_parallel_reduce)(\n",
    "            index, item, reduce_function) for index, item in enumerate(generated_list))\n",
    "\n",
    "        \n",
    "        # 9) Once all threads completed, print on the screen \n",
    "        #   `MapReduce Completed` otherwise print `MapReduce Failed`\n",
    "        boolean_results = [boolean for boolean, path in reduce_return_dict_succeed]\n",
    "        \n",
    "        if False in boolean_results:\n",
    "            status = 'Map Reduce Failed'\n",
    "            return status\n",
    "        \n",
    "        reduce_dict = [reduce_dict for boolean, reduce_dict in reduce_return_dict_succeed]\n",
    "    \n",
    "        reduce_df = pd.DataFrame(data= reduce_dict)\n",
    "        status = 'Map Reduce Completed'\n",
    "        \n",
    "        return status\n",
    "\n",
    "    def _run_at_parallel_map(self, i_item_index, i_item, function):\n",
    "        new_path = None\n",
    "        succeed = True\n",
    "        try:\n",
    "\n",
    "            new_path  = os.path.join(map_reduce_folder_names[0],\n",
    "                                    map_regex + str(i_item_index) + csv_ending) \n",
    "\n",
    "            dict_result = function(i_item)\n",
    "            result_df = pd.DataFrame(data=dict_result, columns = ['key', 'value'])\n",
    "\n",
    "            result_df.to_csv(new_path) \n",
    "\n",
    "            succeed = os.path.exists(new_path)\n",
    "        except:\n",
    "            succeed = False\n",
    "\n",
    "        return  succeed, new_path\n",
    "\n",
    "\n",
    "    def _run_at_parallel_reduce(self, i_item_index, i_item, function):\n",
    "        new_path = None\n",
    "        succeed = True\n",
    "        try:\n",
    "            new_path  = os.path.join(map_reduce_folder_names[1],\n",
    "                                     reduce_regex_init + str(i_item_index) + reduce_regex_final + csv_ending)\n",
    "            #print(new_path)\n",
    "            key = i_item[0]\n",
    "            value = i_item[1]\n",
    "\n",
    "            dict_result = function(key, value)\n",
    "            result_df = pd.DataFrame.from_records([dict_result])\n",
    "\n",
    "            result_df.to_csv(new_path) \n",
    "\n",
    "            succeed = os.path.exists(new_path)\n",
    "        except:\n",
    "            succeed = False\n",
    "\n",
    "        return  succeed, new_path\n",
    "\n",
    "\n",
    "    def _get_grouped_info_from_db_by_key(self, key):\n",
    "        con = sqlite3.connect(db_file_name)\n",
    "        cur = con.cursor()\n",
    "        return_list = []\n",
    "        for row in cur.execute('SELECT key, GROUP_CONCAT(value) FROM ' +db_table_name+ ' GROUP BY ' + key + ' ORDER BY ' + key):\n",
    "            #print(row)\n",
    "            return_list.append(row)\n",
    "        con.close()\n",
    "        return return_list\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>\n",
    "\n",
    "# Question 3\n",
    "## Implement the MapReduce Inverted index of the JSON documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a function `inverted_map(document_name)` which reads the CSV document from the local disc and return a list that contains entries of the form (key_value, document name).\n",
    "\n",
    "For example, if myCSV4.csv document has values like:  \n",
    "`{‘firstname’:’John’,‘secondname’:’Rambo’,‘city’:’Palo Alto’}`\n",
    "\n",
    "Then `inverted_map(‘myCSV4.csv’)` function will return a list:  \n",
    "`[(‘firstname_John’,’ myCSV4.csv’),(‘secondname_Rambo’,’ myCSV4.csv’), (‘city_Palo Alto’,’ myCSV4.csv’)]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverted_map(document_name):\n",
    "    #ducument_name = input_data[0]\n",
    "    csv_df = pd.read_csv(document_name, index_col=0)\n",
    "    csv_size = csv_df.shape[0]\n",
    "    csv_columns = csv_df.columns.to_list()\n",
    "    output_list = []\n",
    "    for i_col in csv_columns:\n",
    "        col_vals  = csv_df[i_col].to_list()\n",
    "        curr_ouput = list(map(lambda x,y,z: (x+ '_' + y, z) , \\\n",
    "                              csv_size*[i_col], col_vals, csv_size*[document_name]))\n",
    "        output_list += curr_ouput\n",
    "    \n",
    "    return output_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a reduce function `inverted_reduce(value, documents)`, where the field “documents” contains a list of all CSV documents per given value.   \n",
    "This list might have duplicates.   \n",
    "Reduce function will return new list without duplicates.\n",
    "\n",
    "For example,  \n",
    "calling the function `inverted_reduce(‘firstname_Albert’,’myCSV2.csv, myCSV5.csv,myCSV2.csv’)`   \n",
    "will return a list `[‘firstname_Albert’,’myCSV2.csv, myCSV5.csv,myCSV2.csv’]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverted_reduce(value, documents):\n",
    "    ducument_name_list = documents.split(',')\n",
    "    ducument_name_list_no_duplicates = list(set(ducument_name_list))\n",
    "    string_ducument_name_list_no_duplicates = (', ').join(ducument_name_list_no_duplicates)\n",
    "    return_list = [value, string_ducument_name_list_no_duplicates]\n",
    "    return return_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>\n",
    "# Question 4\n",
    "## Testing Your MapReduce\n",
    "\n",
    "**Create Python list** `input_data` : `[‘myCSV1.csv’,.. ,‘myCSV20.csv’]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_data:\n",
      "['myCSV1.csv', 'myCSV0.csv', 'myCSV2.csv', 'myCSV3.csv', 'myCSV7.csv', 'myCSV6.csv', 'myCSV4.csv', 'myCSV5.csv', 'myCSV19.csv', 'myCSV18.csv', 'myCSV10.csv', 'myCSV11.csv', 'myCSV13.csv', 'myCSV12.csv', 'myCSV16.csv', 'myCSV17.csv', 'myCSV15.csv', 'myCSV14.csv', 'myCSV8.csv', 'myCSV9.csv']\n"
     ]
    }
   ],
   "source": [
    "input_data = glob.glob(csv_file_name+'*'+csv_ending)\n",
    "print(f'input_data:\\n{input_data}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Submit MapReduce as follows:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Map Reduce Completed\n"
     ]
    }
   ],
   "source": [
    "mapreduce = MapReduceEngine()\n",
    "status = mapreduce.execute(input_data, inverted_map, inverted_reduce)\n",
    "print(status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure that `MapReduce Completed` should be printed and `mapreducefinal` folder should contain the result files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use python to delete all temporary data from mapreducetemp folder and delete SQLite database:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# remove temporary folders & files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete all csv files\n",
    "for i_file in input_data:\n",
    "    if os.path.exists(i_file):\n",
    "        os.remove(i_file)\n",
    "if os.path.exists(db_file_name):\n",
    "    os.remove(db_file_name)\n",
    "for i_folder in map_reduce_folder_names:\n",
    "    if os.path.exists(i_folder):\n",
    "        shutil.rmtree(i_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>\n",
    "\n",
    "# Question 5\n",
    "# Final Thoughts\n",
    "\n",
    "The phase where `MapReduceEngine` reads all temporary files generated by maps and sort them to provide each reducer a specific key is called the **shuffle step**.\n",
    "\n",
    "Please explain **clearly** what would be the main problem of MapReduce when processing Big Data, if there is no shuffle step at all, meaning reducers will directly read responses from the mappers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The main problem of MapReduce if there is no shuffle step is:\n",
    "\n",
    "> **the reduce step will not be performed as expected (we could say it failed)**.\n",
    "\n",
    "> See image below (from the lecture):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://1.bp.blogspot.com/-UvgLSDv7Rb4/Tbpn3veAOTI/AAAAAAAAAVk/kdaMzLa50BE/s1600/WordCountFlow.JPG\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "We can see in the example diagram above that if a reduce worker will recieve the output of the map worker directly, then the reduce function will return the wrong output.\n",
    "<br>\n",
    "for example:\n",
    "the bottom map worker will output:\n",
    "\n",
    " - apple,1\n",
    " \n",
    " - apple,1\n",
    " \n",
    " - plum,1\n",
    " \n",
    "the bottom reduce worker will recieve this as input, and might output:\n",
    "\n",
    " - apple,2\n",
    " \n",
    " - plum,1\n",
    " \n",
    "<br>\n",
    "After the reduce worker finished there is no other reduce step.\n",
    "<br>\n",
    "And so, If we will do the same for all map and reduce workers, we will get the following WRONG unreduced and unsummed output:\n",
    "\n",
    " - apple,1\n",
    " \n",
    " - orange,1\n",
    " \n",
    " - mango,1\n",
    " \n",
    " - orange,1\n",
    " \n",
    " - graspes,1\n",
    " \n",
    " - plum,1\n",
    " \n",
    " - apple,1\n",
    " \n",
    " - plum,1\n",
    " \n",
    " - mango,1\n",
    " \n",
    " - apple,2\n",
    " \n",
    " - plum,1\n",
    " \n",
    "<br>\n",
    "which is obviously not the wanted output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>\n",
    "Thank You :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
